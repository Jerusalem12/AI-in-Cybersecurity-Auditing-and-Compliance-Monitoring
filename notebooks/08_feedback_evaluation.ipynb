{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 08 ¬∑ Feedback Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purpose\n",
    "\n",
    "Run the unified pipeline on auditor feedback data to generate predictions and evaluate model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inputs\n",
    "\n",
    "- `data/raw/round_feedback_full.csv` with artifact_id, text, evidence_type, timestamp, gold_controls, gold_rationale, auditor_id\n",
    "- Trained models: `models/bm25/`, `models/bi_encoder/`, `models/cross_encoder/`, `models/calibration/`, `models/cardinality/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outputs\n",
    "\n",
    "- `data/processed/feedback.csv` with predictions, accept/reject decisions, and accuracy metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps\n",
    "\n",
    "1. Load feedback data and all trained models\n",
    "2. Run unified prediction pipeline on feedback artifacts (same as notebook 06)\n",
    "3. Compare predictions against gold controls\n",
    "4. Calculate accuracy metrics and accept/reject decisions\n",
    "5. Save enriched feedback dataset with predictions and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acceptance Checks\n",
    "\n",
    "- `data/processed/feedback.csv` exists with required columns\n",
    "- Accuracy metrics computed for all feedback artifacts\n",
    "- Accept/reject decisions recorded based on accuracy threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Imports complete\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder, util\n",
    "from rank_bm25 import BM25Okapi\n",
    "import re\n",
    "\n",
    "# Set random seed\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "print(\"‚úì Imports complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load all trained models and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "‚úì Loaded 34 enhanced controls\n",
      "‚úì Loaded 350 feedback artifacts\n",
      "\n",
      "  Sample:\n",
      "  artifact_id evidence_type gold_controls\n",
      "0        2000           log     AC-7;AU-6\n",
      "1        2001        config   SC-28;SC-12\n",
      "2        2002           log          AU-8\n",
      "\n",
      "‚úì Loaded BM25 index\n",
      "‚úì Loaded bi-encoder\n",
      "‚úì Loaded control embeddings: (34, 768)\n",
      "‚úì Loaded cross-encoder\n",
      "‚úì Loaded calibrator\n",
      "‚úì Loaded Auto-K cardinality model\n",
      "\n",
      "============================================================\n",
      "ALL MODELS LOADED\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Configuration (same as notebook 06)\n",
    "RETRIEVE_K = 64  # Initial retrieval\n",
    "RERANK_K = 32    # Rerank top-K\n",
    "BM25_WEIGHT = 0.4\n",
    "BI_ENCODER_WEIGHT = 0.6\n",
    "CROSS_ENCODER_WEIGHT = 0.7\n",
    "FUSED_WEIGHT = 0.3\n",
    "MIN_PROB_THRESHOLD = 0.35\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load enhanced controls\n",
    "controls = pd.read_csv(\"../data/processed/controls_enhanced.csv\", dtype=str)\n",
    "# index_text is already created in controls_enhanced.csv\n",
    "print(f\"‚úì Loaded {len(controls)} enhanced controls\")\n",
    "\n",
    "# Load FEEDBACK data\n",
    "feedback = pd.read_csv(\"../data/raw/round_feedback_full.csv\", dtype={\"artifact_id\": str})\n",
    "print(f\"‚úì Loaded {len(feedback)} feedback artifacts\")\n",
    "print(f\"\\n  Sample:\")\n",
    "print(feedback.head(3)[[\"artifact_id\", \"evidence_type\", \"gold_controls\"]].to_string())\n",
    "\n",
    "# Load BM25 index\n",
    "with open(\"../models/bm25/bm25_index.pkl\", \"rb\") as f:\n",
    "    bm25_data = pickle.load(f)\n",
    "    bm25 = bm25_data[\"bm25\"]\n",
    "print(f\"\\n‚úì Loaded BM25 index\")\n",
    "\n",
    "# Load bi-encoder\n",
    "bi_encoder = SentenceTransformer(\"../models/bi_encoder\", device=device)\n",
    "print(f\"‚úì Loaded bi-encoder\")\n",
    "\n",
    "# Load pre-computed control embeddings\n",
    "control_embeddings = np.load(\"../models/bi_encoder/control_embeddings.npy\")\n",
    "print(f\"‚úì Loaded control embeddings: {control_embeddings.shape}\")\n",
    "\n",
    "# Load cross-encoder\n",
    "cross_encoder = CrossEncoder(\"../models/cross_encoder\", device=device)\n",
    "print(f\"‚úì Loaded cross-encoder\")\n",
    "\n",
    "# Load calibrator\n",
    "with open(\"../models/calibration/cross_iso.pkl\", \"rb\") as f:\n",
    "    calibrator = pickle.load(f)\n",
    "print(f\"‚úì Loaded calibrator\")\n",
    "\n",
    "# Load Auto-K model\n",
    "with open(\"../models/cardinality/model.pkl\", \"rb\") as f:\n",
    "    cardinality_data = pickle.load(f)\n",
    "    cardinality_classifier = cardinality_data[\"classifier\"]\n",
    "    cardinality_scaler = cardinality_data[\"scaler\"]\n",
    "    feature_columns = cardinality_data[\"feature_columns\"]\n",
    "print(f\"‚úì Loaded Auto-K cardinality model\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"ALL MODELS LOADED\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1a. Validate gold controls in feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating gold controls in feedback data...\n",
      "\n",
      "Available controls in catalog: 34\n",
      "  Controls: ['AC-17', 'AC-18', 'AC-2', 'AC-6', 'AC-7', 'AT-2', 'AT-3', 'AU-11', 'AU-12', 'AU-2', 'AU-3', 'AU-6', 'AU-8', 'CM-2', 'CM-3', 'CM-6', 'CM-8', 'CP-9', 'IA-2', 'IA-5', 'IR-4', 'IR-5', 'RA-5', 'SA-11', 'SC-12', 'SC-13', 'SC-28', 'SC-5', 'SC-7', 'SC-8', 'SI-2', 'SI-3', 'SI-4', 'SI-7']\n",
      "\n",
      "Unique controls in feedback gold labels: 32\n",
      "  Controls: ['AC-17', 'AC-18', 'AC-2', 'AC-6', 'AC-7', 'AU-11', 'AU-12', 'AU-2', 'AU-3', 'AU-6', 'AU-8', 'CM-2', 'CM-3', 'CM-6', 'CM-8', 'CP-9', 'IA-2', 'IA-5', 'IR-4', 'IR-5', 'RA-5', 'SA-11', 'SC-12', 'SC-13', 'SC-28', 'SC-5', 'SC-7', 'SC-8', 'SI-2', 'SI-3', 'SI-4', 'SI-7']\n",
      "\n",
      "‚úì All feedback gold controls exist in controls catalog\n",
      "\n",
      "============================================================\n",
      "FEEDBACK GOLD CONTROL STATISTICS\n",
      "============================================================\n",
      "\n",
      "Controls per artifact:\n",
      "  Mean: 1.67\n",
      "  Median: 2\n",
      "  Min: 1\n",
      "  Max: 3\n",
      "\n",
      "  Distribution:\n",
      "gold_controls\n",
      "1    118\n",
      "2    228\n",
      "3      4\n",
      "\n",
      "Most common gold controls in feedback (top 10):\n",
      "  SC-7      :  47 occurrences - Boundary Protection\n",
      "  AC-7      :  46 occurrences - Unsuccessful Logon Attempts\n",
      "  AU-6      :  46 occurrences - Audit Review, Analysis, and Reporting\n",
      "  SC-28     :  46 occurrences - Protection of Information at Rest\n",
      "  AU-8      :  46 occurrences - Time Stamps\n",
      "  SA-11     :  46 occurrences - Developer Testing and Evaluation\n",
      "  CM-3      :  45 occurrences - Configuration Change Control\n",
      "  SC-5      :  44 occurrences - Denial-of-Service Protection\n",
      "  SC-8      :  43 occurrences - Transmission Confidentiality and Integrity\n",
      "  SC-13     :  43 occurrences - Cryptographic Protection\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Validate that all gold controls in feedback exist in controls catalog\n",
    "print(\"Validating gold controls in feedback data...\\n\")\n",
    "\n",
    "# Get all unique control IDs from controls catalog\n",
    "available_controls = set(controls[\"control_id\"].tolist())\n",
    "print(f\"Available controls in catalog: {len(available_controls)}\")\n",
    "print(f\"  Controls: {sorted(available_controls)}\\n\")\n",
    "\n",
    "# Extract all gold controls from feedback\n",
    "def parse_controls_list(control_str):\n",
    "    \"\"\"Parse semicolon-separated control IDs\"\"\"\n",
    "    if pd.isna(control_str) or control_str == \"\":\n",
    "        return []\n",
    "    return str(control_str).split(\";\")\n",
    "\n",
    "all_feedback_controls = set()\n",
    "for _, row in feedback.iterrows():\n",
    "    controls_list = parse_controls_list(row[\"gold_controls\"])\n",
    "    all_feedback_controls.update(controls_list)\n",
    "\n",
    "print(f\"Unique controls in feedback gold labels: {len(all_feedback_controls)}\")\n",
    "print(f\"  Controls: {sorted(all_feedback_controls)}\\n\")\n",
    "\n",
    "# Check for missing controls\n",
    "missing_controls = all_feedback_controls - available_controls\n",
    "if missing_controls:\n",
    "    print(f\"‚ö†Ô∏è  WARNING: {len(missing_controls)} control(s) in feedback NOT found in catalog:\")\n",
    "    print(f\"  Missing: {sorted(missing_controls)}\")\n",
    "    print(f\"\\n  These controls appear in feedback but not in controls.csv.\")\n",
    "    print(f\"  You may need to add them to controls.csv before running predictions.\\n\")\n",
    "else:\n",
    "    print(f\"‚úì All feedback gold controls exist in controls catalog\\n\")\n",
    "\n",
    "# Show statistics\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"FEEDBACK GOLD CONTROL STATISTICS\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Distribution of number of controls per artifact\n",
    "controls_per_artifact = feedback[\"gold_controls\"].apply(lambda x: len(parse_controls_list(x)))\n",
    "print(f\"\\nControls per artifact:\")\n",
    "print(f\"  Mean: {controls_per_artifact.mean():.2f}\")\n",
    "print(f\"  Median: {controls_per_artifact.median():.0f}\")\n",
    "print(f\"  Min: {controls_per_artifact.min()}\")\n",
    "print(f\"  Max: {controls_per_artifact.max()}\")\n",
    "print(f\"\\n  Distribution:\")\n",
    "print(controls_per_artifact.value_counts().sort_index().to_string())\n",
    "\n",
    "# Most common controls in feedback\n",
    "from collections import Counter\n",
    "control_counts = Counter()\n",
    "for _, row in feedback.iterrows():\n",
    "    controls_list = parse_controls_list(row[\"gold_controls\"])\n",
    "    control_counts.update(controls_list)\n",
    "\n",
    "print(f\"\\nMost common gold controls in feedback (top 10):\")\n",
    "for ctrl, count in control_counts.most_common(10):\n",
    "    ctrl_name = controls[controls[\"control_id\"] == ctrl][\"title\"].values[0] if ctrl in available_controls else \"UNKNOWN\"\n",
    "    print(f\"  {ctrl:10s}: {count:3d} occurrences - {ctrl_name}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define unified pipeline functions (same as notebook 06)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Pipeline functions defined\n"
     ]
    }
   ],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"Simple tokenizer for BM25\"\"\"\n",
    "    return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "def hybrid_retrieve(artifact_text, bm25_index, bi_encoder_model, control_embs, top_k=64):\n",
    "    \"\"\"\n",
    "    Step 1: Hybrid retrieval using BM25 + bi-encoder\n",
    "    Returns: List of (control_idx, fused_score) tuples\n",
    "    \"\"\"\n",
    "    # BM25 scores\n",
    "    query_tokens = tokenize(artifact_text)\n",
    "    bm25_scores = bm25_index.get_scores(query_tokens)\n",
    "    bm25_scores_norm = (bm25_scores - bm25_scores.min()) / (bm25_scores.max() - bm25_scores.min() + 1e-10)\n",
    "    \n",
    "    # Bi-encoder scores\n",
    "    artifact_emb = bi_encoder_model.encode(artifact_text, convert_to_tensor=True, device=device)\n",
    "    bi_scores = util.dot_score(artifact_emb, torch.tensor(control_embs, device=device))[0].cpu().numpy()\n",
    "    bi_scores_norm = (bi_scores - bi_scores.min()) / (bi_scores.max() - bi_scores.min() + 1e-10)\n",
    "    \n",
    "    # Fuse scores\n",
    "    fused_scores = BM25_WEIGHT * bm25_scores_norm + BI_ENCODER_WEIGHT * bi_scores_norm\n",
    "    \n",
    "    # Get top-K\n",
    "    top_indices = np.argsort(fused_scores)[::-1][:top_k]\n",
    "    results = [(idx, fused_scores[idx]) for idx in top_indices]\n",
    "    \n",
    "    return results\n",
    "\n",
    "def cross_encoder_rerank(artifact_text, candidate_indices, controls_df, cross_enc, calib, fused_scores, top_k=32):\n",
    "    \"\"\"\n",
    "    Step 2: Cross-encoder reranking with calibrated probabilities\n",
    "    Returns: List of (control_idx, calibrated_prob, final_score) tuples\n",
    "    \"\"\"\n",
    "    # Limit to top_k candidates\n",
    "    candidates = candidate_indices[:top_k]\n",
    "    \n",
    "    # Create pairs for cross-encoder\n",
    "    pairs = [[artifact_text, controls_df.iloc[idx][\"index_text\"]] for idx, _ in candidates]\n",
    "    \n",
    "    # Get cross-encoder scores\n",
    "    ce_scores = cross_enc.predict(pairs, convert_to_numpy=True, show_progress_bar=False)\n",
    "    \n",
    "    # Convert to probabilities\n",
    "    ce_probs = 1 / (1 + np.exp(-ce_scores))\n",
    "    \n",
    "    # Calibrate\n",
    "    calibrated_probs = calib.predict(ce_probs)\n",
    "    \n",
    "    # Get fused scores for blending\n",
    "    fused_vals = np.array([score for _, score in candidates])\n",
    "    fused_norm = (fused_vals - fused_vals.min()) / (fused_vals.max() - fused_vals.min() + 1e-10)\n",
    "    \n",
    "    # Blend: 70% calibrated cross-encoder + 30% fused retrieval\n",
    "    final_scores = CROSS_ENCODER_WEIGHT * calibrated_probs + FUSED_WEIGHT * fused_norm\n",
    "    \n",
    "    # Sort by final score\n",
    "    sorted_indices = np.argsort(final_scores)[::-1]\n",
    "    results = [(candidates[i][0], calibrated_probs[i], final_scores[i]) for i in sorted_indices]\n",
    "    \n",
    "    return results\n",
    "\n",
    "def extract_cardinality_features(calibrated_probs, evidence_type):\n",
    "    \"\"\"\n",
    "    Step 3: Extract features for Auto-K prediction\n",
    "    \"\"\"\n",
    "    # Pad or truncate to 4 scores\n",
    "    scores = np.zeros(4)\n",
    "    scores[:min(4, len(calibrated_probs))] = calibrated_probs[:4]\n",
    "    \n",
    "    features = {\n",
    "        \"s1\": scores[0],\n",
    "        \"s2\": scores[1],\n",
    "        \"s3\": scores[2],\n",
    "        \"s4\": scores[3],\n",
    "        \"delta_12\": scores[0] - scores[1],\n",
    "        \"delta_23\": scores[1] - scores[2],\n",
    "        \"entropy\": -np.sum(calibrated_probs * np.log(calibrated_probs + 1e-10)) if len(calibrated_probs) > 0 else 0.0,\n",
    "        \"type_config\": 1 if evidence_type == \"config\" else 0,\n",
    "        \"type_log\": 1 if evidence_type == \"log\" else 0,\n",
    "        \"type_ticket\": 1 if evidence_type == \"ticket\" else 0\n",
    "    }\n",
    "    \n",
    "    return features\n",
    "\n",
    "def predict_cardinality(features, classifier, scaler, feature_cols):\n",
    "    \"\"\"\n",
    "    Step 4: Predict how many controls to return (1-3)\n",
    "    \"\"\"\n",
    "    # Create feature vector in correct order\n",
    "    X = pd.DataFrame([features])[feature_cols]\n",
    "    X_scaled = scaler.transform(X)\n",
    "    k = classifier.predict(X_scaled)[0]\n",
    "    return k\n",
    "\n",
    "print(\"‚úì Pipeline functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run unified pipeline on feedback data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running unified pipeline on FEEDBACK data...\n",
      "This will take several minutes...\n",
      "\n",
      "Processing 350 feedback artifacts...\n",
      "  Processed 25/350...\n",
      "  Processed 50/350...\n",
      "  Processed 75/350...\n",
      "  Processed 100/350...\n",
      "  Processed 125/350...\n",
      "  Processed 150/350...\n",
      "  Processed 175/350...\n",
      "  Processed 200/350...\n",
      "  Processed 225/350...\n",
      "  Processed 250/350...\n",
      "  Processed 275/350...\n",
      "  Processed 300/350...\n",
      "  Processed 325/350...\n",
      "  Processed 350/350...\n",
      "\n",
      "‚úì Completed 350 predictions\n"
     ]
    }
   ],
   "source": [
    "def predict_for_artifact(row, controls_df):\n",
    "    \"\"\"Run full pipeline for a single artifact\"\"\"\n",
    "    artifact_text = row[\"text\"]\n",
    "    evidence_type = row[\"evidence_type\"]\n",
    "    \n",
    "    # Step 1: Hybrid retrieval\n",
    "    candidates = hybrid_retrieve(artifact_text, bm25, bi_encoder, control_embeddings, top_k=RETRIEVE_K)\n",
    "    \n",
    "    # Step 2: Cross-encoder reranking\n",
    "    reranked = cross_encoder_rerank(\n",
    "        artifact_text, candidates, controls_df, \n",
    "        cross_encoder, calibrator, candidates, top_k=RERANK_K\n",
    "    )\n",
    "    \n",
    "    # Extract calibrated probabilities\n",
    "    calibrated_probs = np.array([prob for _, prob, _ in reranked])\n",
    "    \n",
    "    # Step 3: Extract features for cardinality prediction\n",
    "    features = extract_cardinality_features(calibrated_probs, evidence_type)\n",
    "    \n",
    "    # Step 4: Predict K (number of controls to return)\n",
    "    k = predict_cardinality(features, cardinality_classifier, cardinality_scaler, feature_columns)\n",
    "    \n",
    "    # Select top-K controls (also apply minimum probability threshold)\n",
    "    selected = []\n",
    "    for i, (ctrl_idx, cal_prob, final_score) in enumerate(reranked[:k]):\n",
    "        if cal_prob >= MIN_PROB_THRESHOLD:\n",
    "            selected.append({\n",
    "                \"control_id\": controls_df.iloc[ctrl_idx][\"control_id\"],\n",
    "                \"calibrated_prob\": cal_prob,\n",
    "                \"final_score\": final_score\n",
    "            })\n",
    "    \n",
    "    # If no controls pass threshold, take top 1\n",
    "    if len(selected) == 0 and len(reranked) > 0:\n",
    "        ctrl_idx, cal_prob, final_score = reranked[0]\n",
    "        selected.append({\n",
    "            \"control_id\": controls_df.iloc[ctrl_idx][\"control_id\"],\n",
    "            \"calibrated_prob\": cal_prob,\n",
    "            \"final_score\": final_score\n",
    "        })\n",
    "    \n",
    "    return selected\n",
    "\n",
    "# Run pipeline on feedback data\n",
    "print(\"Running unified pipeline on FEEDBACK data...\")\n",
    "print(\"This will take several minutes...\")\n",
    "print(f\"\\nProcessing {len(feedback)} feedback artifacts...\")\n",
    "\n",
    "predictions = []\n",
    "for idx, row in feedback.iterrows():\n",
    "    selected_controls = predict_for_artifact(row, controls)\n",
    "    \n",
    "    predictions.append({\n",
    "        \"artifact_id\": row[\"artifact_id\"],\n",
    "        \"text\": row[\"text\"],\n",
    "        \"evidence_type\": row[\"evidence_type\"],\n",
    "        \"timestamp\": row[\"timestamp\"],\n",
    "        \"gold_controls\": row[\"gold_controls\"],\n",
    "        \"gold_rationale\": row[\"gold_rationale\"],\n",
    "        \"auditor_id\": row[\"auditor_id\"],\n",
    "        \"predicted_controls\": \";\".join([c[\"control_id\"] for c in selected_controls]),\n",
    "        \"predicted_probs\": \";\".join([f\"{c['calibrated_prob']:.4f}\" for c in selected_controls]),\n",
    "        \"n_predicted\": len(selected_controls)\n",
    "    })\n",
    "    \n",
    "    if (len(predictions) % 25 == 0):\n",
    "        print(f\"  Processed {len(predictions)}/{len(feedback)}...\")\n",
    "\n",
    "print(f\"\\n‚úì Completed {len(predictions)} predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluate predictions and compute accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing accuracy metrics...\n",
      "\n",
      "‚úì Accuracy metrics computed\n",
      "\n",
      "============================================================\n",
      "FEEDBACK EVALUATION SUMMARY\n",
      "============================================================\n",
      "  Total artifacts: 350\n",
      "  Mean accuracy: 0.8719\n",
      "  Median accuracy: 1.0000\n",
      "  Average predictions per artifact: 1.58\n",
      "\n",
      "  Accept/Reject (threshold=0.5):\n",
      "accept_reject\n",
      "accept    334\n",
      "reject     16\n",
      "\n",
      "  Accuracy categories:\n",
      "accuracy_category\n",
      "perfect    270\n",
      "partial     64\n",
      "poor        16\n",
      "\n",
      "  Accuracy by evidence type:\n",
      "    log       : 0.9546 (n=158)\n",
      "    config    : 0.9528 (n=106)\n",
      "    ticket    : 0.6202 (n=86)\n"
     ]
    }
   ],
   "source": [
    "def parse_controls(control_str):\n",
    "    \"\"\"Parse semicolon-separated control IDs\"\"\"\n",
    "    if pd.isna(control_str) or control_str == \"\":\n",
    "        return set()\n",
    "    return set(str(control_str).split(\";\"))\n",
    "\n",
    "def compute_accuracy(gold_set, pred_set):\n",
    "    \"\"\"\n",
    "    Compute Jaccard similarity (intersection over union)\n",
    "    Returns: accuracy value between 0 and 1\n",
    "    \"\"\"\n",
    "    if len(gold_set) == 0 and len(pred_set) == 0:\n",
    "        return 1.0\n",
    "    \n",
    "    intersection = len(gold_set & pred_set)\n",
    "    union = len(gold_set | pred_set)\n",
    "    \n",
    "    if union == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    return intersection / union\n",
    "\n",
    "def determine_accept_reject(accuracy_value, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Determine if prediction is acceptable based on accuracy\n",
    "    Returns: 'accept' or 'reject'\n",
    "    \"\"\"\n",
    "    return \"accept\" if accuracy_value >= threshold else \"reject\"\n",
    "\n",
    "print(\"Computing accuracy metrics...\\n\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "predictions_df = pd.DataFrame(predictions)\n",
    "\n",
    "# Parse control sets\n",
    "predictions_df[\"gold_set\"] = predictions_df[\"gold_controls\"].apply(parse_controls)\n",
    "predictions_df[\"pred_set\"] = predictions_df[\"predicted_controls\"].apply(parse_controls)\n",
    "\n",
    "# Compute accuracy\n",
    "predictions_df[\"accuracy_value\"] = predictions_df.apply(\n",
    "    lambda row: compute_accuracy(row[\"gold_set\"], row[\"pred_set\"]), axis=1\n",
    ")\n",
    "\n",
    "# Determine accept/reject\n",
    "ACCEPTANCE_THRESHOLD = 0.5\n",
    "predictions_df[\"accept_reject\"] = predictions_df[\"accuracy_value\"].apply(\n",
    "    lambda x: determine_accept_reject(x, ACCEPTANCE_THRESHOLD)\n",
    ")\n",
    "\n",
    "# Compute accuracy category (perfect/good/partial/poor)\n",
    "def categorize_accuracy(acc):\n",
    "    if acc == 1.0:\n",
    "        return \"perfect\"\n",
    "    elif acc >= 0.7:\n",
    "        return \"good\"\n",
    "    elif acc >= 0.4:\n",
    "        return \"partial\"\n",
    "    else:\n",
    "        return \"poor\"\n",
    "\n",
    "predictions_df[\"accuracy_category\"] = predictions_df[\"accuracy_value\"].apply(categorize_accuracy)\n",
    "\n",
    "print(\"‚úì Accuracy metrics computed\")\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"FEEDBACK EVALUATION SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"  Total artifacts: {len(predictions_df)}\")\n",
    "print(f\"  Mean accuracy: {predictions_df['accuracy_value'].mean():.4f}\")\n",
    "print(f\"  Median accuracy: {predictions_df['accuracy_value'].median():.4f}\")\n",
    "print(f\"  Average predictions per artifact: {predictions_df['n_predicted'].mean():.2f}\")\n",
    "print(f\"\\n  Accept/Reject (threshold={ACCEPTANCE_THRESHOLD}):\")\n",
    "print(predictions_df[\"accept_reject\"].value_counts().to_string())\n",
    "print(f\"\\n  Accuracy categories:\")\n",
    "print(predictions_df[\"accuracy_category\"].value_counts().to_string())\n",
    "\n",
    "# Per evidence type breakdown\n",
    "print(f\"\\n  Accuracy by evidence type:\")\n",
    "for evidence_type in predictions_df[\"evidence_type\"].unique():\n",
    "    subset = predictions_df[predictions_df[\"evidence_type\"] == evidence_type]\n",
    "    mean_acc = subset[\"accuracy_value\"].mean()\n",
    "    print(f\"    {evidence_type:10s}: {mean_acc:.4f} (n={len(subset)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Show sample comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "SAMPLE PREDICTIONS\n",
      "============================================================\n",
      "\n",
      "PERFECT Match:\n",
      "  Artifact 2000 (log):\n",
      "    Text: Multiple failed logins from 10.0.5.12 to auth-service; account svc-app not locked after 5 attempts....\n",
      "    Gold: ['AC-7', 'AU-6']\n",
      "    Pred: ['AC-7', 'AU-6']\n",
      "    Accuracy: 1.0000\n",
      "    Decision: accept\n",
      "\n",
      "PARTIAL Match:\n",
      "  Artifact 2005 (ticket):\n",
      "    Text: Change deployed to prod without approval link; emergency process not documented....\n",
      "    Gold: ['CM-3', 'SA-11']\n",
      "    Pred: ['CM-3']\n",
      "    Accuracy: 0.5000\n",
      "    Decision: accept\n",
      "\n",
      "POOR Match:\n",
      "  Artifact 10910 (ticket):\n",
      "    Text: A review of the 'Finance' Active Directory group found that a user from the IT department is a membe...\n",
      "    Gold: ['AC-6']\n",
      "    Pred: ['AC-2']\n",
      "    Accuracy: 0.0000\n",
      "    Decision: reject\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"SAMPLE PREDICTIONS\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Show examples from each accuracy category\n",
    "for category in [\"perfect\", \"good\", \"partial\", \"poor\"]:\n",
    "    subset = predictions_df[predictions_df[\"accuracy_category\"] == category]\n",
    "    if len(subset) > 0:\n",
    "        print(f\"\\n{category.upper()} Match:\")\n",
    "        sample = subset.iloc[0]\n",
    "        print(f\"  Artifact {sample['artifact_id']} ({sample['evidence_type']}):\")\n",
    "        print(f\"    Text: {sample['text'][:100]}...\")\n",
    "        print(f\"    Gold: {sorted(sample['gold_set'])}\")\n",
    "        print(f\"    Pred: {sorted(sample['pred_set'])}\")\n",
    "        print(f\"    Accuracy: {sample['accuracy_value']:.4f}\")\n",
    "        print(f\"    Decision: {sample['accept_reject']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save feedback with predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Saved feedback with predictions to ../data/processed/feedback.csv\n",
      "  Rows: 350\n",
      "  Columns: ['artifact_id', 'text', 'evidence_type', 'timestamp', 'gold_controls', 'gold_rationale', 'auditor_id', 'predicted_controls', 'accept_reject', 'accuracy_value']\n",
      "  File size: 84.26 KB\n"
     ]
    }
   ],
   "source": [
    "# Prepare output dataframe with required columns\n",
    "feedback_output = predictions_df[[\n",
    "    \"artifact_id\",\n",
    "    \"text\",\n",
    "    \"evidence_type\",\n",
    "    \"timestamp\",\n",
    "    \"gold_controls\",\n",
    "    \"gold_rationale\",\n",
    "    \"auditor_id\",\n",
    "    \"predicted_controls\",\n",
    "    \"accept_reject\",\n",
    "    \"accuracy_value\"\n",
    "]].copy()\n",
    "\n",
    "# Create output directory\n",
    "output_dir = Path(\"../data/processed\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save feedback with predictions\n",
    "output_path = output_dir / \"feedback.csv\"\n",
    "feedback_output.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"‚úì Saved feedback with predictions to {output_path}\")\n",
    "print(f\"  Rows: {len(feedback_output)}\")\n",
    "print(f\"  Columns: {list(feedback_output.columns)}\")\n",
    "print(f\"  File size: {output_path.stat().st_size / 1024:.2f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Acceptance checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ACCEPTANCE CHECKS\n",
      "============================================================\n",
      "\n",
      "‚úì Check 1: feedback.csv exists with required columns\n",
      "  Path: ../data/processed/feedback.csv\n",
      "  Exists: True\n",
      "  Required columns present: True\n",
      "  Result: PASS\n",
      "\n",
      "‚úì Check 2: Accuracy metrics computed for all artifacts\n",
      "  Total artifacts: 350\n",
      "  All have accuracy_value: True\n",
      "  Accuracy range: [0.0000, 1.0000]\n",
      "  Result: PASS\n",
      "\n",
      "‚úì Check 3: Accept/reject decisions recorded\n",
      "  All have accept_reject: True\n",
      "  Valid values only: True\n",
      "  Distribution: {'accept': np.int64(334), 'reject': np.int64(16)}\n",
      "  Result: PASS\n",
      "\n",
      "============================================================\n",
      "‚úÖ ALL ACCEPTANCE CHECKS PASSED\n",
      "\n",
      "Notebook 08 completed successfully!\n",
      "Feedback data saved to: ../data/processed/feedback.csv\n",
      "\n",
      "Next steps:\n",
      "  - Review rejected predictions for model improvement\n",
      "  - Use feedback.csv for continuous learning and retraining\n",
      "  - Analyze patterns in errors by evidence type and control family\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"ACCEPTANCE CHECKS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check 1: feedback.csv exists and has required columns\n",
    "required_columns = [\n",
    "    \"artifact_id\", \"text\", \"evidence_type\", \"timestamp\",\n",
    "    \"gold_controls\", \"gold_rationale\", \"auditor_id\",\n",
    "    \"predicted_controls\", \"accept_reject\", \"accuracy_value\"\n",
    "]\n",
    "check1 = output_path.exists() and all(col in feedback_output.columns for col in required_columns)\n",
    "print(f\"\\n‚úì Check 1: feedback.csv exists with required columns\")\n",
    "print(f\"  Path: {output_path}\")\n",
    "print(f\"  Exists: {output_path.exists()}\")\n",
    "print(f\"  Required columns present: {all(col in feedback_output.columns for col in required_columns)}\")\n",
    "print(f\"  Result: {'PASS' if check1 else 'FAIL'}\")\n",
    "\n",
    "# Check 2: Accuracy metrics computed for all artifacts\n",
    "check2 = len(feedback_output) > 0 and feedback_output[\"accuracy_value\"].notna().all()\n",
    "print(f\"\\n‚úì Check 2: Accuracy metrics computed for all artifacts\")\n",
    "print(f\"  Total artifacts: {len(feedback_output)}\")\n",
    "print(f\"  All have accuracy_value: {feedback_output['accuracy_value'].notna().all()}\")\n",
    "print(f\"  Accuracy range: [{feedback_output['accuracy_value'].min():.4f}, {feedback_output['accuracy_value'].max():.4f}]\")\n",
    "print(f\"  Result: {'PASS' if check2 else 'FAIL'}\")\n",
    "\n",
    "# Check 3: Accept/reject decisions recorded\n",
    "check3 = feedback_output[\"accept_reject\"].notna().all() and set(feedback_output[\"accept_reject\"].unique()).issubset({\"accept\", \"reject\"})\n",
    "print(f\"\\n‚úì Check 3: Accept/reject decisions recorded\")\n",
    "print(f\"  All have accept_reject: {feedback_output['accept_reject'].notna().all()}\")\n",
    "print(f\"  Valid values only: {set(feedback_output['accept_reject'].unique()).issubset({'accept', 'reject'})}\")\n",
    "print(f\"  Distribution: {dict(feedback_output['accept_reject'].value_counts())}\")\n",
    "print(f\"  Result: {'PASS' if check3 else 'FAIL'}\")\n",
    "\n",
    "# Overall\n",
    "all_checks_passed = check1 and check2 and check3\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "if all_checks_passed:\n",
    "    print(\"‚úÖ ALL ACCEPTANCE CHECKS PASSED\")\n",
    "    print(\"\\nNotebook 08 completed successfully!\")\n",
    "    print(f\"Feedback data saved to: {output_path}\")\n",
    "    print(f\"\\nNext steps:\")\n",
    "    print(f\"  - Review rejected predictions for model improvement\")\n",
    "    print(f\"  - Use feedback.csv for continuous learning and retraining\")\n",
    "    print(f\"  - Analyze patterns in errors by evidence type and control family\")\n",
    "else:\n",
    "    print(\"‚ùå SOME ACCEPTANCE CHECKS FAILED\")\n",
    "    print(\"\\nPlease review the failed checks above\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Interactive Feedback Collection\n",
    "\n",
    "Collect new predictions and feedback from users interactively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive Feedback Collection\n",
    "\n",
    "**This cell provides a user-friendly interface for collecting feedback on predictions.**\n",
    "\n",
    "#### How to Use:\n",
    "\n",
    "1. **Enter artifact details** in the form fields:\n",
    "   - **Artifact**: The operational evidence text (log entry, config snippet, or ticket description)\n",
    "   - **Evidence Type**: Select log, config, or ticket from dropdown\n",
    "   - **Rationale**: (Optional) Explanation of why certain controls apply\n",
    "   - **Auditor ID**: Identifier for the person providing feedback (defaults to \"interactive\")\n",
    "\n",
    "2. **Click \"Get Prediction\"** button to run the model\n",
    "\n",
    "3. **Review the prediction**:\n",
    "   - Model will show predicted controls with confidence scores\n",
    "   - Each control displays its ID, title, and confidence percentage\n",
    "\n",
    "4. **Provide feedback** by clicking one of two buttons:\n",
    "   - **‚úì Correct**: If prediction matches the correct controls ‚Üí saves prediction as gold standard\n",
    "   - **‚úó Incorrect**: If prediction is wrong ‚Üí enter correct control IDs in the text field that appears\n",
    "\n",
    "5. **For incorrect predictions**:\n",
    "   - Enter correct controls in format: `AC-7;AU-6` (semicolon-separated)\n",
    "   - Click \"Submit Correction\" to save\n",
    "   - System automatically calculates accuracy and accept/reject status\n",
    "\n",
    "6. **Repeat**: Form automatically resets after each submission\n",
    "\n",
    "#### What Happens:\n",
    "\n",
    "- Each entry is saved to `data/processed/feedback.csv` with:\n",
    "  - Auto-generated artifact ID\n",
    "  - Timestamp\n",
    "  - Predicted vs. actual controls\n",
    "  - Accuracy score (Jaccard similarity)\n",
    "  - Accept/reject decision\n",
    "\n",
    "- **All new feedback is immediately available** for retraining with notebook 09\n",
    "\n",
    "#### Run the cell below to start the interactive interface!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "INTERACTIVE FEEDBACK COLLECTION\n",
      "============================================================\n",
      "\n",
      "Enter artifact details and click 'Get Prediction':\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bf7820a250e4bc998832fa491c7d5cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='', description='Artifact:', layout=Layout(height='100px', width='90%'), placeholder='Enter art‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d08fd9ebf785481c8ae3b0a9cf1d2a2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Evidence Type:', options=('log', 'config', 'ticket'), value='log')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90e39b896a0745fd8a5e7480d8503620",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='', description='Rationale:', layout=Layout(height='60px', width='90%'), placeholder='Optional ‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2697b0b8fc78439cb2126f58f48f236e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='interactive', description='Auditor ID:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "998af0a7a0d649a393c73985752e7da2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='primary', description='Get Prediction', icon='search', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "815269ebd58c43728d5f0bc0a7287a16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5961a83d46e4ac98bdd8912f3c95a22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Button(button_style='success', description='‚úì Correct', icon='check', layout=Layout(display='no‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73db1fcae3524d1fba88eb2da98a35eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='Correct IDs:', layout=Layout(display='none', width='90%'), placeholder='Enter corr‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5537758d21b54341aa3e7e91b2591e5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='warning', description='Submit Correction', layout=Layout(display='none'), style=ButtonSty‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90a0573a918f4705b5cca880fa2030e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Interactive prediction and feedback collection using ipywidgets\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "from datetime import datetime\n",
    "\n",
    "# Create interactive widgets\n",
    "artifact_text_widget = widgets.Textarea(\n",
    "    placeholder='Enter artifact text here...',\n",
    "    description='Artifact:',\n",
    "    layout=widgets.Layout(width='90%', height='100px')\n",
    ")\n",
    "\n",
    "evidence_type_widget = widgets.Dropdown(\n",
    "    options=['log', 'config', 'ticket'],\n",
    "    value='log',\n",
    "    description='Evidence Type:',\n",
    ")\n",
    "\n",
    "gold_rationale_widget = widgets.Textarea(\n",
    "    placeholder='Optional explanation...',\n",
    "    description='Rationale:',\n",
    "    layout=widgets.Layout(width='90%', height='60px')\n",
    ")\n",
    "\n",
    "auditor_id_widget = widgets.Text(\n",
    "    value='interactive',\n",
    "    description='Auditor ID:',\n",
    ")\n",
    "\n",
    "predict_button = widgets.Button(\n",
    "    description='Get Prediction',\n",
    "    button_style='primary',\n",
    "    icon='search'\n",
    ")\n",
    "\n",
    "prediction_output = widgets.Output()\n",
    "\n",
    "# Feedback widgets (hidden initially)\n",
    "correct_button = widgets.Button(\n",
    "    description='‚úì Correct',\n",
    "    button_style='success',\n",
    "    icon='check',\n",
    "    layout=widgets.Layout(display='none')\n",
    ")\n",
    "\n",
    "incorrect_button = widgets.Button(\n",
    "    description='‚úó Incorrect',\n",
    "    button_style='danger',\n",
    "    icon='times',\n",
    "    layout=widgets.Layout(display='none')\n",
    ")\n",
    "\n",
    "correct_controls_widget = widgets.Text(\n",
    "    placeholder='Enter correct controls (e.g., AC-7;AU-6)',\n",
    "    description='Correct IDs:',\n",
    "    layout=widgets.Layout(width='90%', display='none')\n",
    ")\n",
    "\n",
    "submit_correction_button = widgets.Button(\n",
    "    description='Submit Correction',\n",
    "    button_style='warning',\n",
    "    layout=widgets.Layout(display='none')\n",
    ")\n",
    "\n",
    "feedback_output = widgets.Output()\n",
    "\n",
    "# State variables\n",
    "current_prediction = {}\n",
    "\n",
    "def get_next_artifact_id():\n",
    "    \"\"\"Get the next available artifact ID\"\"\"\n",
    "    existing_feedback = pd.read_csv(\"../data/processed/feedback.csv\")\n",
    "    return int(existing_feedback[\"artifact_id\"].max()) + 1\n",
    "\n",
    "def on_predict_clicked(b):\n",
    "    \"\"\"Handle predict button click\"\"\"\n",
    "    global current_prediction\n",
    "    \n",
    "    with prediction_output:\n",
    "        clear_output()\n",
    "        \n",
    "        # Validate inputs\n",
    "        if not artifact_text_widget.value.strip():\n",
    "            print(\"‚ùå Error: Artifact text cannot be empty\")\n",
    "            return\n",
    "        \n",
    "        # Create artifact row\n",
    "        artifact_row = {\n",
    "            \"text\": artifact_text_widget.value.strip(),\n",
    "            \"evidence_type\": evidence_type_widget.value\n",
    "        }\n",
    "        \n",
    "        print(\"üîÑ Running prediction...\")\n",
    "        \n",
    "        # Run prediction\n",
    "        selected_controls = predict_for_artifact(artifact_row, controls)\n",
    "        predicted_controls = [c[\"control_id\"] for c in selected_controls]\n",
    "        predicted_probs = [c[\"calibrated_prob\"] for c in selected_controls]\n",
    "        \n",
    "        # Store current prediction\n",
    "        current_prediction = {\n",
    "            \"artifact_text\": artifact_text_widget.value.strip(),\n",
    "            \"evidence_type\": evidence_type_widget.value,\n",
    "            \"gold_rationale\": gold_rationale_widget.value.strip(),\n",
    "            \"auditor_id\": auditor_id_widget.value.strip() or \"interactive\",\n",
    "            \"predicted_controls\": predicted_controls,\n",
    "            \"predicted_probs\": predicted_probs\n",
    "        }\n",
    "        \n",
    "        # Display prediction\n",
    "        print(f\"\\n‚úÖ Predicted controls:\\n\")\n",
    "        for i, (ctrl_id, prob) in enumerate(zip(predicted_controls, predicted_probs), 1):\n",
    "            ctrl_title = controls[controls[\"control_id\"] == ctrl_id][\"title\"].values[0]\n",
    "            print(f\"  {i}. {ctrl_id} - {ctrl_title}\")\n",
    "            print(f\"     Confidence: {prob:.2%}\\n\")\n",
    "        \n",
    "        print(\"=\"*60)\n",
    "        print(\"Is this prediction correct?\")\n",
    "        \n",
    "        # Show feedback buttons\n",
    "        correct_button.layout.display = 'inline-block'\n",
    "        incorrect_button.layout.display = 'inline-block'\n",
    "\n",
    "def on_correct_clicked(b):\n",
    "    \"\"\"Handle correct button click\"\"\"\n",
    "    with feedback_output:\n",
    "        clear_output()\n",
    "        \n",
    "        # Prediction is correct\n",
    "        gold_controls = \";\".join(current_prediction[\"predicted_controls\"])\n",
    "        accept_reject = \"accept\"\n",
    "        accuracy_value = 1.0\n",
    "        \n",
    "        # Save to feedback.csv\n",
    "        save_feedback(gold_controls, accept_reject, accuracy_value)\n",
    "        \n",
    "        print(\"‚úÖ Feedback recorded as CORRECT\")\n",
    "        print(f\"   Saved to feedback.csv\")\n",
    "        \n",
    "        # Reset form\n",
    "        reset_form()\n",
    "\n",
    "def on_incorrect_clicked(b):\n",
    "    \"\"\"Handle incorrect button click\"\"\"\n",
    "    with feedback_output:\n",
    "        clear_output()\n",
    "        print(\"Please enter the correct control IDs below:\")\n",
    "    \n",
    "    # Show correction input\n",
    "    correct_controls_widget.layout.display = 'block'\n",
    "    submit_correction_button.layout.display = 'inline-block'\n",
    "    \n",
    "    # Hide feedback buttons\n",
    "    correct_button.layout.display = 'none'\n",
    "    incorrect_button.layout.display = 'none'\n",
    "\n",
    "def on_submit_correction_clicked(b):\n",
    "    \"\"\"Handle submit correction button click\"\"\"\n",
    "    with feedback_output:\n",
    "        clear_output()\n",
    "        \n",
    "        gold_controls_input = correct_controls_widget.value.strip()\n",
    "        \n",
    "        if not gold_controls_input:\n",
    "            print(\"‚ùå Error: Must provide correct controls\")\n",
    "            return\n",
    "        \n",
    "        # Compute accuracy\n",
    "        gold_set = parse_controls(gold_controls_input)\n",
    "        pred_set = set(current_prediction[\"predicted_controls\"])\n",
    "        accuracy_value = compute_accuracy(gold_set, pred_set)\n",
    "        accept_reject = determine_accept_reject(accuracy_value, ACCEPTANCE_THRESHOLD)\n",
    "        \n",
    "        # Save to feedback.csv\n",
    "        save_feedback(gold_controls_input, accept_reject, accuracy_value)\n",
    "        \n",
    "        print(\"‚úÖ Feedback recorded as INCORRECT\")\n",
    "        print(f\"   Accuracy: {accuracy_value:.2%}\")\n",
    "        print(f\"   Decision: {accept_reject}\")\n",
    "        print(f\"   Saved to feedback.csv\")\n",
    "        \n",
    "        # Reset form\n",
    "        reset_form()\n",
    "\n",
    "def save_feedback(gold_controls, accept_reject, accuracy_value):\n",
    "    \"\"\"Save feedback entry to CSV\"\"\"\n",
    "    next_id = get_next_artifact_id()\n",
    "    \n",
    "    # Create new feedback entry\n",
    "    new_feedback = {\n",
    "        \"artifact_id\": str(next_id),\n",
    "        \"text\": current_prediction[\"artifact_text\"],\n",
    "        \"evidence_type\": current_prediction[\"evidence_type\"],\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"gold_controls\": gold_controls,\n",
    "        \"gold_rationale\": current_prediction[\"gold_rationale\"],\n",
    "        \"auditor_id\": current_prediction[\"auditor_id\"],\n",
    "        \"predicted_controls\": \";\".join(current_prediction[\"predicted_controls\"]),\n",
    "        \"accept_reject\": accept_reject,\n",
    "        \"accuracy_value\": accuracy_value\n",
    "    }\n",
    "    \n",
    "    # Append to feedback.csv\n",
    "    existing_feedback = pd.read_csv(\"../data/processed/feedback.csv\")\n",
    "    new_feedback_df = pd.DataFrame([new_feedback])\n",
    "    updated_feedback = pd.concat([existing_feedback, new_feedback_df], ignore_index=True)\n",
    "    updated_feedback.to_csv(\"../data/processed/feedback.csv\", index=False)\n",
    "    \n",
    "    with feedback_output:\n",
    "        print(f\"\\n   Artifact ID: {next_id}\")\n",
    "        print(f\"   Total feedback entries: {len(updated_feedback)}\")\n",
    "\n",
    "def reset_form():\n",
    "    \"\"\"Reset form to initial state\"\"\"\n",
    "    # Clear text fields\n",
    "    artifact_text_widget.value = ''\n",
    "    gold_rationale_widget.value = ''\n",
    "    correct_controls_widget.value = ''\n",
    "    \n",
    "    # Hide buttons\n",
    "    correct_button.layout.display = 'none'\n",
    "    incorrect_button.layout.display = 'none'\n",
    "    correct_controls_widget.layout.display = 'none'\n",
    "    submit_correction_button.layout.display = 'none'\n",
    "    \n",
    "    # Clear outputs\n",
    "    with prediction_output:\n",
    "        clear_output()\n",
    "\n",
    "# Attach event handlers\n",
    "predict_button.on_click(on_predict_clicked)\n",
    "correct_button.on_click(on_correct_clicked)\n",
    "incorrect_button.on_click(on_incorrect_clicked)\n",
    "submit_correction_button.on_click(on_submit_correction_clicked)\n",
    "\n",
    "# Display the interface\n",
    "print(\"=\"*60)\n",
    "print(\"INTERACTIVE FEEDBACK COLLECTION\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nEnter artifact details and click 'Get Prediction':\\n\")\n",
    "\n",
    "display(artifact_text_widget)\n",
    "display(evidence_type_widget)\n",
    "display(gold_rationale_widget)\n",
    "display(auditor_id_widget)\n",
    "display(predict_button)\n",
    "display(prediction_output)\n",
    "display(widgets.HBox([correct_button, incorrect_button]))\n",
    "display(correct_controls_widget)\n",
    "display(submit_correction_button)\n",
    "display(feedback_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
