{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b758bb1b",
   "metadata": {},
   "source": [
    "# 03 — Feedback Effects\n",
    "Simulate auditor feedback from train, re‑score, and compare metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e72f430",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from crs.dataio import load_artifacts, load_controls\n",
    "from crs.controls import build_index_text\n",
    "from crs.recommenders.tfidf import TFIDFRecommender\n",
    "from crs.metrics import top1_accuracy, precision_at_k, recall_at_k, jaccard\n",
    "from crs.feedback import learn_from_train\n",
    "\n",
    "CFG_PATH = Path('../configs/defaults.yaml')\n",
    "cfg = yaml.safe_load(CFG_PATH.read_text())\n",
    "\n",
    "controls = load_controls(cfg['paths']['controls'])\n",
    "artifacts = load_artifacts(cfg['paths']['artifacts'])\n",
    "index_texts = build_index_text(controls)\n",
    "\n",
    "rec = TFIDFRecommender(\n",
    "    ngram_range=tuple(cfg.get('tfidf', {}).get('ngram_range', [1,2])),\n",
    "    min_df=cfg.get('tfidf', {}).get('min_df', 1)\n",
    ").fit(index_texts, controls['control_id'].tolist())\n",
    "\n",
    "k = cfg.get('k', 3)\n",
    "train = artifacts[artifacts['split']=='train'].copy()\n",
    "test  = artifacts[artifacts['split']=='test'].copy()\n",
    "\n",
    "# Baseline\n",
    "rows=[]\n",
    "for _, r in test.iterrows():\n",
    "    ids, scores = rec.predict_topk(r['text'], k=k)\n",
    "    rows.append({'artifact_id': int(r['artifact_id']), 'gold_controls': r['gold_controls'],\n",
    "                 'predicted_topk': ';'.join(ids), 'scores_topk': ';'.join(f\"{s:.4f}\" for s in scores)})\n",
    "preds_base = pd.DataFrame(rows)\n",
    "\n",
    "# Learn boosts/negatives\n",
    "boosts, negatives = learn_from_train(train, rec, rounds=1, alpha=0.03, beta=0.01)\n",
    "\n",
    "# Feedback variant\n",
    "rows_fb=[]\n",
    "for _, r in test.iterrows():\n",
    "    ids, scores = rec.predict_topk(r['text'], k=k, boosts=boosts, negatives=negatives)\n",
    "    rows_fb.append({'artifact_id': int(r['artifact_id']), 'gold_controls': r['gold_controls'],\n",
    "                    'predicted_topk': ';'.join(ids), 'scores_topk': ';'.join(f\"{s:.4f}\" for s in scores)})\n",
    "preds_fb = pd.DataFrame(rows_fb)\n",
    "\n",
    "def metrics_row(df, tag):\n",
    "    return {\n",
    "        'tag': tag,\n",
    "        'top1': round(top1_accuracy(df), 3),\n",
    "        f'P@{k}': round(precision_at_k(df, k=k), 3),\n",
    "        f'R@{k}': round(recall_at_k(df, k=k), 3),\n",
    "        f'J@{k}': round(jaccard(df, k=k), 3),\n",
    "    }\n",
    "\n",
    "m = pd.DataFrame([metrics_row(preds_base, 'baseline'), metrics_row(preds_fb, 'feedback')])\n",
    "display(m)\n",
    "\n",
    "plt.figure()\n",
    "plt.bar(m['tag'], m[f'P@{k}'])\n",
    "plt.title('Precision@{}: Baseline vs Feedback'.format(k))\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Precision')\n",
    "plt.tight_layout()\n",
    "\n",
    "out_dir = Path('../outputs/predictions'); out_dir.mkdir(parents=True, exist_ok=True)\n",
    "preds_base.to_csv(out_dir/'test_baseline.csv', index=False)\n",
    "preds_fb.to_csv(out_dir/'test_feedback.csv', index=False)\n",
    "print('Saved:', out_dir/'test_baseline.csv', 'and', out_dir/'test_feedback.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}