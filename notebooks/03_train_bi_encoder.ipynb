{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 03 \u00b7 Train Bi-Encoder Retriever"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Purpose\n\nFine-tune a sentence-transformer bi-encoder for semantic retrieval over controls."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inputs\n\n- `data/processed/pairs/train.jsonl` for training instances.\n- `data/processed/pairs/dev.jsonl` for validation metrics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Outputs\n\n- `models/bi_encoder/` directory with fine-tuned weights, tokenizer, and training config.\n- Optional precomputed control embeddings array (e.g., `controls_embeddings.npy`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Steps\n\n1. Load the base checkpoint `sentence-transformers/multi-qa-mpnet-base-dot-v1`.\n2. Set up MultipleNegativesRankingLoss with batch size 64 (gradient accumulation if needed).\n3. Train for 3\u20135 epochs with learning rate 2e-5 and 10% warmup, logging progress.\n4. Evaluate on the dev split to compute MRR@10 and select the best checkpoint.\n5. Persist the tuned model and optionally encode the control catalog for faster inference."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Acceptance Checks\n\n- `models/bi_encoder/` contains the saved model artifacts.\n- Validation MRR@10 is computed and reported for the best checkpoint."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
