{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d7b7d09",
   "metadata": {},
   "source": [
    "# 01 · Data Prep and Splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9df4f4",
   "metadata": {},
   "source": [
    "## Purpose\n",
    "\n",
    "Prepare artifact splits without leakage before downstream training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eaa43ab",
   "metadata": {},
   "source": [
    "## Inputs\n",
    "\n",
    "- `data/raw/controls.csv` — canonical NIST control catalog (control_id,family,title,summary).\n",
    "- `data/raw/artifacts.csv` — raw artifacts with optional `partition`, labels, and rationale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a44668",
   "metadata": {},
   "source": [
    "## Outputs\n",
    "\n",
    "- `data/processed/artifacts_with_split.csv` with validated schema and enforced partitions.\n",
    "- Inline partition and evidence-type summary tables for sanity checks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf4c057",
   "metadata": {},
   "source": [
    "## Steps\n",
    "\n",
    "1. Load raw CSVs, assert required columns, and coerce types (IDs as strings, timestamps as datetime).\n",
    "2. If `partition` is missing or blank, assign stratified 60/20/20 (train/dev/test) using a fixed RNG seed.\n",
    "3. Normalize artifact text (lowercase + trim) to build a leakage hash and detect duplicates.\n",
    "4. Resolve duplicates so each unique text appears in exactly one split (prefer train when conflicts).\n",
    "5. Drop helper hash columns, persist the processed CSV, and surface partition/evidence counts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c2d9bc",
   "metadata": {},
   "source": [
    "## Acceptance Checks\n",
    "\n",
    "- Only `train`, `dev`, and `test` appear in the partition column.\n",
    "- Zero duplicate normalized texts across different partitions.\n",
    "- `data/processed/artifacts_with_split.csv` exists on disk after execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "y7jh086hhb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-26T02:11:18.331674Z",
     "iopub.status.busy": "2025-10-26T02:11:18.331541Z",
     "iopub.status.idle": "2025-10-26T02:11:19.326480Z",
     "shell.execute_reply": "2025-10-26T02:11:19.326188Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import hashlib\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "x0wz7fwoqle",
   "metadata": {},
   "source": [
    "## 1. Load and validate raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8qxhur1u8tf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-26T02:11:19.327996Z",
     "iopub.status.busy": "2025-10-26T02:11:19.327885Z",
     "iopub.status.idle": "2025-10-26T02:11:19.340204Z",
     "shell.execute_reply": "2025-10-26T02:11:19.339990Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded 31 controls\n",
      "  Columns: ['control_id', 'family', 'title', 'summary']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>control_id</th>\n",
       "      <th>family</th>\n",
       "      <th>title</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AC-2</td>\n",
       "      <td>AC</td>\n",
       "      <td>Account Management</td>\n",
       "      <td>Provision, review, and remove accounts; enforc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AC-6</td>\n",
       "      <td>AC</td>\n",
       "      <td>Least Privilege</td>\n",
       "      <td>Restrict privileges to the minimum necessary; ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AC-7</td>\n",
       "      <td>AC</td>\n",
       "      <td>Unsuccessful Logon Attempts</td>\n",
       "      <td>Enforce lockout thresholds and durations after...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  control_id family                        title  \\\n",
       "0       AC-2     AC           Account Management   \n",
       "1       AC-6     AC              Least Privilege   \n",
       "2       AC-7     AC  Unsuccessful Logon Attempts   \n",
       "\n",
       "                                             summary  \n",
       "0  Provision, review, and remove accounts; enforc...  \n",
       "1  Restrict privileges to the minimum necessary; ...  \n",
       "2  Enforce lockout thresholds and durations after...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load controls\n",
    "controls_path = Path(\"../data/raw/controls.csv\")\n",
    "controls = pd.read_csv(controls_path, dtype=str)\n",
    "\n",
    "# Validate required columns\n",
    "required_control_cols = [\"control_id\", \"family\", \"title\", \"summary\"]\n",
    "assert all(col in controls.columns for col in required_control_cols), \\\n",
    "    f\"Missing columns in controls.csv. Expected: {required_control_cols}\"\n",
    "\n",
    "print(f\"✓ Loaded {len(controls)} controls\")\n",
    "print(f\"  Columns: {list(controls.columns)}\")\n",
    "controls.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "vumpp2ji9me",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-26T02:11:19.341382Z",
     "iopub.status.busy": "2025-10-26T02:11:19.341307Z",
     "iopub.status.idle": "2025-10-26T02:11:19.351138Z",
     "shell.execute_reply": "2025-10-26T02:11:19.350949Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded 1000 artifacts\n",
      "  Columns: ['artifact_id', 'text', 'evidence_type', 'timestamp', 'gold_controls', 'gold_rationale']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artifact_id</th>\n",
       "      <th>text</th>\n",
       "      <th>evidence_type</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>gold_controls</th>\n",
       "      <th>gold_rationale</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10522</td>\n",
       "      <td>Asset inventory shows 22 untagged cloud instan...</td>\n",
       "      <td>config</td>\n",
       "      <td>2025-09-18 05:20:00+00:00</td>\n",
       "      <td>CM-8</td>\n",
       "      <td>Information system component inventory incompl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10738</td>\n",
       "      <td>API gateway rate limiting configured; DDoS pro...</td>\n",
       "      <td>config</td>\n",
       "      <td>2025-11-11 09:00:00+00:00</td>\n",
       "      <td>SC-5;SC-7</td>\n",
       "      <td>Traffic filtering implemented to prevent DoS a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10741</td>\n",
       "      <td>SSH daemon configuration hardened; root login ...</td>\n",
       "      <td>config</td>\n",
       "      <td>2025-11-11 09:45:00+00:00</td>\n",
       "      <td>AC-17;SC-8</td>\n",
       "      <td>Remote access security enhanced with strong cr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  artifact_id                                               text  \\\n",
       "0       10522  Asset inventory shows 22 untagged cloud instan...   \n",
       "1       10738  API gateway rate limiting configured; DDoS pro...   \n",
       "2       10741  SSH daemon configuration hardened; root login ...   \n",
       "\n",
       "  evidence_type                 timestamp gold_controls  \\\n",
       "0        config 2025-09-18 05:20:00+00:00          CM-8   \n",
       "1        config 2025-11-11 09:00:00+00:00     SC-5;SC-7   \n",
       "2        config 2025-11-11 09:45:00+00:00    AC-17;SC-8   \n",
       "\n",
       "                                      gold_rationale  \n",
       "0  Information system component inventory incompl...  \n",
       "1  Traffic filtering implemented to prevent DoS a...  \n",
       "2  Remote access security enhanced with strong cr...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load artifacts\n",
    "artifacts_path = Path(\"../data/raw/artifacts.csv\")\n",
    "artifacts = pd.read_csv(artifacts_path, dtype={\"artifact_id\": str, \"text\": str, \"evidence_type\": str, \"gold_controls\": str, \"gold_rationale\": str})\n",
    "\n",
    "# Parse timestamp\n",
    "artifacts[\"timestamp\"] = pd.to_datetime(artifacts[\"timestamp\"], errors=\"coerce\")\n",
    "\n",
    "# Validate required columns\n",
    "required_artifact_cols = [\"artifact_id\", \"text\", \"evidence_type\", \"gold_controls\"]\n",
    "assert all(col in artifacts.columns for col in required_artifact_cols), \\\n",
    "    f\"Missing columns in artifacts.csv. Expected: {required_artifact_cols}\"\n",
    "\n",
    "print(f\"✓ Loaded {len(artifacts)} artifacts\")\n",
    "print(f\"  Columns: {list(artifacts.columns)}\")\n",
    "artifacts.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jlt99xsbgur",
   "metadata": {},
   "source": [
    "## 2. Assign train/dev/test partitions (60/20/20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "hoes68nj9v",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-26T02:11:19.352240Z",
     "iopub.status.busy": "2025-10-26T02:11:19.352158Z",
     "iopub.status.idle": "2025-10-26T02:11:19.357121Z",
     "shell.execute_reply": "2025-10-26T02:11:19.356950Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ No partition column found or all values are missing. Assigning 60/20/20 split...\n",
      "\n",
      "✓ Partition distribution:\n",
      "partition\n",
      "dev      200\n",
      "test     200\n",
      "train    600\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check if partition column exists\n",
    "if \"partition\" not in artifacts.columns or artifacts[\"partition\"].isna().all():\n",
    "    print(\"⚠ No partition column found or all values are missing. Assigning 60/20/20 split...\")\n",
    "    \n",
    "    # Shuffle and split\n",
    "    n = len(artifacts)\n",
    "    indices = np.random.permutation(n)\n",
    "    \n",
    "    train_end = int(0.6 * n)\n",
    "    dev_end = int(0.8 * n)\n",
    "    \n",
    "    partitions = np.empty(n, dtype=object)\n",
    "    partitions[indices[:train_end]] = \"train\"\n",
    "    partitions[indices[train_end:dev_end]] = \"dev\"\n",
    "    partitions[indices[dev_end:]] = \"test\"\n",
    "    \n",
    "    artifacts[\"partition\"] = partitions\n",
    "else:\n",
    "    # Fill missing partitions with train/dev/test\n",
    "    print(\"ℹ Partition column exists. Filling missing values...\")\n",
    "    artifacts[\"partition\"] = artifacts[\"partition\"].fillna(\"train\")\n",
    "\n",
    "print(f\"\\n✓ Partition distribution:\")\n",
    "print(artifacts[\"partition\"].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4sbnliiybzs",
   "metadata": {},
   "source": [
    "## 3. Detect and resolve duplicate texts across partitions (leakage guard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "gitdzst42c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-26T02:11:19.358146Z",
     "iopub.status.busy": "2025-10-26T02:11:19.358079Z",
     "iopub.status.idle": "2025-10-26T02:11:19.381873Z",
     "shell.execute_reply": "2025-10-26T02:11:19.381632Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 86 unique texts appearing in multiple partitions\n",
      "\n",
      "⚠ Resolving duplicates (keeping first occurrence, preferring train)...\n",
      "  Removed 95 duplicate rows\n"
     ]
    }
   ],
   "source": [
    "# Create normalized text hash for duplicate detection\n",
    "artifacts[\"text_hash\"] = artifacts[\"text\"].str.lower().str.strip().apply(\n",
    "    lambda x: hashlib.md5(x.encode()).hexdigest() if pd.notna(x) else None\n",
    ")\n",
    "\n",
    "# Find duplicates across partitions\n",
    "duplicates = artifacts.groupby(\"text_hash\")[\"partition\"].nunique()\n",
    "cross_partition_dupes = duplicates[duplicates > 1]\n",
    "\n",
    "print(f\"Found {len(cross_partition_dupes)} unique texts appearing in multiple partitions\")\n",
    "\n",
    "if len(cross_partition_dupes) > 0:\n",
    "    print(\"\\n⚠ Resolving duplicates (keeping first occurrence, preferring train)...\")\n",
    "    \n",
    "    # For each duplicate hash, keep only one partition (prefer train > dev > test)\n",
    "    partition_priority = {\"train\": 0, \"dev\": 1, \"test\": 2}\n",
    "    \n",
    "    # Mark rows to keep\n",
    "    artifacts[\"_priority\"] = artifacts[\"partition\"].map(partition_priority)\n",
    "    artifacts[\"_keep\"] = False\n",
    "    \n",
    "    for text_hash in cross_partition_dupes.index:\n",
    "        mask = artifacts[\"text_hash\"] == text_hash\n",
    "        dupe_group = artifacts[mask].sort_values(\"_priority\")\n",
    "        # Keep only the first (highest priority) occurrence\n",
    "        first_idx = dupe_group.index[0]\n",
    "        artifacts.loc[first_idx, \"_keep\"] = True\n",
    "    \n",
    "    # Also keep all non-duplicates\n",
    "    non_dupe_mask = ~artifacts[\"text_hash\"].isin(cross_partition_dupes.index)\n",
    "    artifacts.loc[non_dupe_mask, \"_keep\"] = True\n",
    "    \n",
    "    # Remove duplicates\n",
    "    rows_before = len(artifacts)\n",
    "    artifacts = artifacts[artifacts[\"_keep\"]].copy()\n",
    "    rows_after = len(artifacts)\n",
    "    \n",
    "    print(f\"  Removed {rows_before - rows_after} duplicate rows\")\n",
    "    \n",
    "    # Clean up helper columns\n",
    "    artifacts.drop(columns=[\"_priority\", \"_keep\"], inplace=True)\n",
    "else:\n",
    "    print(\"✓ No cross-partition duplicates found!\")\n",
    "\n",
    "# Drop the hash column\n",
    "artifacts.drop(columns=[\"text_hash\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wdt3lz06459",
   "metadata": {},
   "source": [
    "## 4. Save processed data and print summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "mguvtb9m21c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-26T02:11:19.382975Z",
     "iopub.status.busy": "2025-10-26T02:11:19.382907Z",
     "iopub.status.idle": "2025-10-26T02:11:19.388899Z",
     "shell.execute_reply": "2025-10-26T02:11:19.388724Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved 905 artifacts to ../data/processed/artifacts_with_split.csv\n",
      "\n",
      "Final partition distribution:\n",
      "partition\n",
      "dev      162\n",
      "test     148\n",
      "train    595\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create output directory if needed\n",
    "output_dir = Path(\"../data/processed\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save processed artifacts\n",
    "output_path = output_dir / \"artifacts_with_split.csv\"\n",
    "artifacts.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"✓ Saved {len(artifacts)} artifacts to {output_path}\")\n",
    "print(f\"\\nFinal partition distribution:\")\n",
    "print(artifacts[\"partition\"].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "l1i057442w",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-26T02:11:19.389974Z",
     "iopub.status.busy": "2025-10-26T02:11:19.389912Z",
     "iopub.status.idle": "2025-10-26T02:11:19.399274Z",
     "shell.execute_reply": "2025-10-26T02:11:19.399096Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Evidence Type × Partition Distribution\n",
      "============================================================\n",
      "partition      dev  test  train  All\n",
      "evidence_type                       \n",
      "config          57    57    194  308\n",
      "log             56    48    216  320\n",
      "ticket          49    43    185  277\n",
      "All            162   148    595  905\n"
     ]
    }
   ],
   "source": [
    "# Evidence type × partition crosstab\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Evidence Type × Partition Distribution\")\n",
    "print(\"=\"*60)\n",
    "crosstab = pd.crosstab(artifacts[\"evidence_type\"], artifacts[\"partition\"], margins=True)\n",
    "print(crosstab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cz4qpe97t9o",
   "metadata": {},
   "source": [
    "## 5. Acceptance checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "n435n0luvto",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-26T02:11:19.400384Z",
     "iopub.status.busy": "2025-10-26T02:11:19.400307Z",
     "iopub.status.idle": "2025-10-26T02:11:19.405764Z",
     "shell.execute_reply": "2025-10-26T02:11:19.405587Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ACCEPTANCE CHECKS\n",
      "============================================================\n",
      "\n",
      "✓ Check 1: Only train/dev/test partitions: True\n",
      "  Found partitions: ['dev', 'test', 'train']\n",
      "\n",
      "✓ Check 2: Zero cross-partition duplicates: True\n",
      "  Duplicate texts across partitions: 0\n",
      "\n",
      "✓ Check 3: Output file exists: True\n",
      "  Path: ../data/processed/artifacts_with_split.csv\n",
      "\n",
      "============================================================\n",
      "✅ ALL ACCEPTANCE CHECKS PASSED\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"ACCEPTANCE CHECKS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check 1: Only train/dev/test partitions\n",
    "valid_partitions = {\"train\", \"dev\", \"test\"}\n",
    "actual_partitions = set(artifacts[\"partition\"].unique())\n",
    "check1 = actual_partitions.issubset(valid_partitions)\n",
    "print(f\"\\n✓ Check 1: Only train/dev/test partitions: {check1}\")\n",
    "print(f\"  Found partitions: {sorted(actual_partitions)}\")\n",
    "\n",
    "# Check 2: Zero cross-partition duplicates\n",
    "artifacts_recheck = pd.read_csv(output_path)\n",
    "artifacts_recheck[\"text_hash\"] = artifacts_recheck[\"text\"].str.lower().str.strip().apply(\n",
    "    lambda x: hashlib.md5(x.encode()).hexdigest() if pd.notna(x) else None\n",
    ")\n",
    "duplicates_final = artifacts_recheck.groupby(\"text_hash\")[\"partition\"].nunique()\n",
    "cross_partition_final = (duplicates_final > 1).sum()\n",
    "check2 = cross_partition_final == 0\n",
    "print(f\"\\n✓ Check 2: Zero cross-partition duplicates: {check2}\")\n",
    "print(f\"  Duplicate texts across partitions: {cross_partition_final}\")\n",
    "\n",
    "# Check 3: Output file exists\n",
    "check3 = output_path.exists()\n",
    "print(f\"\\n✓ Check 3: Output file exists: {check3}\")\n",
    "print(f\"  Path: {output_path}\")\n",
    "\n",
    "# Overall\n",
    "all_checks_passed = check1 and check2 and check3\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "if all_checks_passed:\n",
    "    print(\"✅ ALL ACCEPTANCE CHECKS PASSED\")\n",
    "else:\n",
    "    print(\"❌ SOME ACCEPTANCE CHECKS FAILED\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
