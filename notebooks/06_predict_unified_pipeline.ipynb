{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06 · Unified Inference Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purpose\n",
    "\n",
    "Run the end-to-end pipeline (BM25 + bi-encoder fusion, cross-encoder rerank, Auto-K outputs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inputs\n",
    "\n",
    "- `data/processed/artifacts_with_split.csv` filtered to the desired partition.\n",
    "- Trained assets: `models/bm25/` (optional cache), `models/bi_encoder/`, `models/cross_encoder/`, calibration file, and Auto-K classifier.\n",
    "- `configs/predict_hybrid.yaml` with runtime parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outputs\n",
    "\n",
    "- `outputs/predictions/test.csv` containing ranked control predictions for held-out test artifacts.\n",
    "- `outputs/predictions/dev.csv` optional sanity predictions for dev (if executed)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps\n",
    "\n",
    "1. Load configuration values (paths, retrieval depths, fusion weights, thresholds).\n",
    "2. Retrieve candidate controls by fusing BM25 and bi-encoder scores; retain the top N for reranking.\n",
    "3. Apply the cross-encoder to rerank candidates and calibrate probabilities.\n",
    "4. Feed artifact-level features into the Auto-K model to decide how many controls (1–3) to emit.\n",
    "5. Write prediction files with artifact_id, selected controls, probabilities, and supporting fields."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acceptance Checks\n",
    "\n",
    "- Average number of predicted controls falls between 1 and ~2.x with all lengths in {1,2,3}.\n",
    "- Gold labels are not referenced during inference (test partition only).\n",
    "- `outputs/predictions/test.csv` is produced at the end of the run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "gjols67awps",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Imports complete\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder, util\n",
    "from rank_bm25 import BM25Okapi\n",
    "import re\n",
    "\n",
    "# Set random seed\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "print(\"✓ Imports complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sm5vap1ycc",
   "metadata": {},
   "source": [
    "## 1. Load all trained models and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbj138duag8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "✓ Loaded 34 enhanced controls\n",
      "✓ Loaded 2574 artifacts\n",
      "✓ Loaded BM25 index\n",
      "✓ Loaded bi-encoder\n",
      "✓ Loaded control embeddings: (34, 768)\n",
      "✓ Loaded cross-encoder\n",
      "✓ Loaded calibrator\n",
      "✓ Loaded Auto-K cardinality model\n",
      "\n",
      "============================================================\n",
      "ALL MODELS LOADED\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "RETRIEVE_K = 64  # Initial retrieval\n",
    "RERANK_K = 32    # Rerank top-K\n",
    "BM25_WEIGHT = 0.4\n",
    "BI_ENCODER_WEIGHT = 0.6\n",
    "CROSS_ENCODER_WEIGHT = 0.7\n",
    "FUSED_WEIGHT = 0.3\n",
    "MIN_PROB_THRESHOLD = 0.35\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load enhanced controls\n",
    "controls = pd.read_csv(\"../data/processed/controls_enhanced.csv\", dtype=str)\n",
    "# index_text is already created in controls_enhanced.csv\n",
    "print(f\"✓ Loaded {len(controls)} enhanced controls\")\n",
    "\n",
    "# Load artifacts\n",
    "artifacts = pd.read_csv(\"../data/processed/artifacts_with_split.csv\", dtype={\"artifact_id\": str})\n",
    "print(f\"✓ Loaded {len(artifacts)} artifacts\")\n",
    "\n",
    "# Load BM25 index\n",
    "with open(\"../models/bm25/bm25_index.pkl\", \"rb\") as f:\n",
    "    bm25_data = pickle.load(f)\n",
    "    bm25 = bm25_data[\"bm25\"]\n",
    "print(f\"✓ Loaded BM25 index\")\n",
    "\n",
    "# Load bi-encoder\n",
    "bi_encoder = SentenceTransformer(\"../models/bi_encoder\", device=device)\n",
    "print(f\"✓ Loaded bi-encoder\")\n",
    "\n",
    "# Load pre-computed control embeddings\n",
    "control_embeddings = np.load(\"../models/bi_encoder/control_embeddings.npy\")\n",
    "print(f\"✓ Loaded control embeddings: {control_embeddings.shape}\")\n",
    "\n",
    "# Load cross-encoder\n",
    "cross_encoder = CrossEncoder(\"../models/cross_encoder\", device=device)\n",
    "print(f\"✓ Loaded cross-encoder\")\n",
    "\n",
    "# Load calibrator\n",
    "with open(\"../models/calibration/cross_iso.pkl\", \"rb\") as f:\n",
    "    calibrator = pickle.load(f)\n",
    "print(f\"✓ Loaded calibrator\")\n",
    "\n",
    "# Load Auto-K model\n",
    "with open(\"../models/cardinality/model.pkl\", \"rb\") as f:\n",
    "    cardinality_data = pickle.load(f)\n",
    "    cardinality_classifier = cardinality_data[\"classifier\"]\n",
    "    cardinality_scaler = cardinality_data[\"scaler\"]\n",
    "    feature_columns = cardinality_data[\"feature_columns\"]\n",
    "print(f\"✓ Loaded Auto-K cardinality model\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"ALL MODELS LOADED\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eg0pw9w5xyh",
   "metadata": {},
   "source": [
    "## 2. Define unified pipeline functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7lv58zlzbsj",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Pipeline functions defined\n"
     ]
    }
   ],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"Simple tokenizer for BM25\"\"\"\n",
    "    return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "def hybrid_retrieve(artifact_text, bm25_index, bi_encoder_model, control_embs, top_k=64):\n",
    "    \"\"\"\n",
    "    Step 1: Hybrid retrieval using BM25 + bi-encoder\n",
    "    Returns: List of (control_idx, fused_score) tuples\n",
    "    \"\"\"\n",
    "    # BM25 scores\n",
    "    query_tokens = tokenize(artifact_text)\n",
    "    bm25_scores = bm25_index.get_scores(query_tokens)\n",
    "    bm25_scores_norm = (bm25_scores - bm25_scores.min()) / (bm25_scores.max() - bm25_scores.min() + 1e-10)\n",
    "    \n",
    "    # Bi-encoder scores\n",
    "    artifact_emb = bi_encoder_model.encode(artifact_text, convert_to_tensor=True, device=device)\n",
    "    bi_scores = util.dot_score(artifact_emb, torch.tensor(control_embs, device=device))[0].cpu().numpy()\n",
    "    bi_scores_norm = (bi_scores - bi_scores.min()) / (bi_scores.max() - bi_scores.min() + 1e-10)\n",
    "    \n",
    "    # Fuse scores\n",
    "    fused_scores = BM25_WEIGHT * bm25_scores_norm + BI_ENCODER_WEIGHT * bi_scores_norm\n",
    "    \n",
    "    # Get top-K\n",
    "    top_indices = np.argsort(fused_scores)[::-1][:top_k]\n",
    "    results = [(idx, fused_scores[idx]) for idx in top_indices]\n",
    "    \n",
    "    return results\n",
    "\n",
    "def cross_encoder_rerank(artifact_text, candidate_indices, controls_df, cross_enc, calib, fused_scores, top_k=32):\n",
    "    \"\"\"\n",
    "    Step 2: Cross-encoder reranking with calibrated probabilities\n",
    "    Returns: List of (control_idx, calibrated_prob, final_score) tuples\n",
    "    \"\"\"\n",
    "    # Limit to top_k candidates\n",
    "    candidates = candidate_indices[:top_k]\n",
    "    \n",
    "    # Create pairs for cross-encoder\n",
    "    pairs = [[artifact_text, controls_df.iloc[idx][\"index_text\"]] for idx, _ in candidates]\n",
    "    \n",
    "    # Get cross-encoder scores\n",
    "    ce_scores = cross_enc.predict(pairs, convert_to_numpy=True, show_progress_bar=False)\n",
    "    \n",
    "    # Convert to probabilities\n",
    "    ce_probs = 1 / (1 + np.exp(-ce_scores))\n",
    "    \n",
    "    # Calibrate\n",
    "    calibrated_probs = calib.predict(ce_probs)\n",
    "    \n",
    "    # Get fused scores for blending\n",
    "    fused_vals = np.array([score for _, score in candidates])\n",
    "    fused_norm = (fused_vals - fused_vals.min()) / (fused_vals.max() - fused_vals.min() + 1e-10)\n",
    "    \n",
    "    # Blend: 70% calibrated cross-encoder + 30% fused retrieval\n",
    "    final_scores = CROSS_ENCODER_WEIGHT * calibrated_probs + FUSED_WEIGHT * fused_norm\n",
    "    \n",
    "    # Sort by final score\n",
    "    sorted_indices = np.argsort(final_scores)[::-1]\n",
    "    results = [(candidates[i][0], calibrated_probs[i], final_scores[i]) for i in sorted_indices]\n",
    "    \n",
    "    return results\n",
    "\n",
    "def extract_cardinality_features(calibrated_probs, evidence_type):\n",
    "    \"\"\"\n",
    "    Step 3: Extract features for Auto-K prediction\n",
    "    \"\"\"\n",
    "    # Pad or truncate to 4 scores\n",
    "    scores = np.zeros(4)\n",
    "    scores[:min(4, len(calibrated_probs))] = calibrated_probs[:4]\n",
    "    \n",
    "    features = {\n",
    "        \"s1\": scores[0],\n",
    "        \"s2\": scores[1],\n",
    "        \"s3\": scores[2],\n",
    "        \"s4\": scores[3],\n",
    "        \"delta_12\": scores[0] - scores[1],\n",
    "        \"delta_23\": scores[1] - scores[2],\n",
    "        \"entropy\": -np.sum(calibrated_probs * np.log(calibrated_probs + 1e-10)) if len(calibrated_probs) > 0 else 0.0,\n",
    "        \"type_config\": 1 if evidence_type == \"config\" else 0,\n",
    "        \"type_log\": 1 if evidence_type == \"log\" else 0,\n",
    "        \"type_ticket\": 1 if evidence_type == \"ticket\" else 0\n",
    "    }\n",
    "    \n",
    "    return features\n",
    "\n",
    "def predict_cardinality(features, classifier, scaler, feature_cols):\n",
    "    \"\"\"\n",
    "    Step 4: Predict how many controls to return (1-3)\n",
    "    \"\"\"\n",
    "    # Create feature vector in correct order\n",
    "    X = pd.DataFrame([features])[feature_cols]\n",
    "    X_scaled = scaler.transform(X)\n",
    "    k = classifier.predict(X_scaled)[0]\n",
    "    return k\n",
    "\n",
    "print(\"✓ Pipeline functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3697aikwdrc",
   "metadata": {},
   "source": [
    "## 3. Run unified pipeline on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "egra15pwtd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running unified pipeline on TEST set...\n",
      "This will take several minutes...\n",
      "\n",
      "Processing 354 test artifacts...\n",
      "  Processed 25/354...\n",
      "  Processed 50/354...\n",
      "  Processed 75/354...\n",
      "  Processed 100/354...\n",
      "  Processed 125/354...\n",
      "  Processed 150/354...\n",
      "  Processed 175/354...\n",
      "  Processed 200/354...\n",
      "  Processed 225/354...\n",
      "  Processed 250/354...\n",
      "  Processed 275/354...\n",
      "  Processed 300/354...\n",
      "  Processed 325/354...\n",
      "  Processed 350/354...\n",
      "\n",
      "✓ Completed 354 predictions\n"
     ]
    }
   ],
   "source": [
    "def predict_for_artifact(row, controls_df):\n",
    "    \"\"\"Run full pipeline for a single artifact\"\"\"\n",
    "    artifact_text = row[\"text\"]\n",
    "    evidence_type = row[\"evidence_type\"]\n",
    "    \n",
    "    # Step 1: Hybrid retrieval\n",
    "    candidates = hybrid_retrieve(artifact_text, bm25, bi_encoder, control_embeddings, top_k=RETRIEVE_K)\n",
    "    \n",
    "    # Step 2: Cross-encoder reranking\n",
    "    reranked = cross_encoder_rerank(\n",
    "        artifact_text, candidates, controls_df, \n",
    "        cross_encoder, calibrator, candidates, top_k=RERANK_K\n",
    "    )\n",
    "    \n",
    "    # Extract calibrated probabilities\n",
    "    calibrated_probs = np.array([prob for _, prob, _ in reranked])\n",
    "    \n",
    "    # Step 3: Extract features for cardinality prediction\n",
    "    features = extract_cardinality_features(calibrated_probs, evidence_type)\n",
    "    \n",
    "    # Step 4: Predict K (number of controls to return)\n",
    "    k = predict_cardinality(features, cardinality_classifier, cardinality_scaler, feature_columns)\n",
    "    \n",
    "    # Select top-K controls (also apply minimum probability threshold)\n",
    "    selected = []\n",
    "    for i, (ctrl_idx, cal_prob, final_score) in enumerate(reranked[:k]):\n",
    "        if cal_prob >= MIN_PROB_THRESHOLD:\n",
    "            selected.append({\n",
    "                \"control_id\": controls_df.iloc[ctrl_idx][\"control_id\"],\n",
    "                \"calibrated_prob\": cal_prob,\n",
    "                \"final_score\": final_score\n",
    "            })\n",
    "    \n",
    "    # If no controls pass threshold, take top 1\n",
    "    if len(selected) == 0 and len(reranked) > 0:\n",
    "        ctrl_idx, cal_prob, final_score = reranked[0]\n",
    "        selected.append({\n",
    "            \"control_id\": controls_df.iloc[ctrl_idx][\"control_id\"],\n",
    "            \"calibrated_prob\": cal_prob,\n",
    "            \"final_score\": final_score\n",
    "        })\n",
    "    \n",
    "    return selected\n",
    "\n",
    "# Run pipeline on test set\n",
    "print(\"Running unified pipeline on TEST set...\")\n",
    "print(\"This will take several minutes...\")\n",
    "\n",
    "test_artifacts = artifacts[artifacts[\"partition\"] == \"test\"].copy()\n",
    "print(f\"\\nProcessing {len(test_artifacts)} test artifacts...\")\n",
    "\n",
    "predictions = []\n",
    "for idx, row in test_artifacts.iterrows():\n",
    "    selected_controls = predict_for_artifact(row, controls)\n",
    "    \n",
    "    predictions.append({\n",
    "        \"artifact_id\": row[\"artifact_id\"],\n",
    "        \"text\": row[\"text\"],\n",
    "        \"evidence_type\": row[\"evidence_type\"],\n",
    "        \"gold_controls\": row[\"gold_controls\"],\n",
    "        \"predicted_controls\": \";\".join([c[\"control_id\"] for c in selected_controls]),\n",
    "        \"predicted_probs\": \";\".join([f\"{c['calibrated_prob']:.4f}\" for c in selected_controls]),\n",
    "        \"n_predicted\": len(selected_controls)\n",
    "    })\n",
    "    \n",
    "    if (len(predictions) % 25 == 0):\n",
    "        print(f\"  Processed {len(predictions)}/{len(test_artifacts)}...\")\n",
    "\n",
    "print(f\"\\n✓ Completed {len(predictions)} predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gax3znavvr6",
   "metadata": {},
   "source": [
    "## 4. Save predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "oyoikxwqwjn",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved 354 predictions to ../outputs/predictions/test.csv\n",
      "\n",
      "Prediction summary:\n",
      "  Average controls per artifact: 1.51\n",
      "  Distribution of K:\n",
      "n_predicted\n",
      "1    192\n",
      "2    144\n",
      "3     18\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample predictions:\n",
      "  artifact_id predicted_controls predicted_probs  n_predicted\n",
      "0       10002        SC-12;SC-28   0.9691;0.9412            2\n",
      "1       10005         SC-13;SC-8   0.9905;0.8977            2\n",
      "2       10017               AC-6          0.8977            1\n",
      "3       10020               AU-2          0.7073            1\n",
      "4       10022               SI-3          0.9412            1\n"
     ]
    }
   ],
   "source": [
    "# Create output directory\n",
    "output_dir = Path(\"../outputs/predictions\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save predictions\n",
    "predictions_df = pd.DataFrame(predictions)\n",
    "output_path = output_dir / \"test.csv\"\n",
    "predictions_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"✓ Saved {len(predictions_df)} predictions to {output_path}\")\n",
    "print(f\"\\nPrediction summary:\")\n",
    "print(f\"  Average controls per artifact: {predictions_df['n_predicted'].mean():.2f}\")\n",
    "print(f\"  Distribution of K:\")\n",
    "print(predictions_df[\"n_predicted\"].value_counts().sort_index())\n",
    "\n",
    "# Show sample predictions\n",
    "print(f\"\\nSample predictions:\")\n",
    "print(predictions_df[[\"artifact_id\", \"predicted_controls\", \"predicted_probs\", \"n_predicted\"]].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6gan41e4dt7",
   "metadata": {},
   "source": [
    "## 5. Acceptance checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "mqyb5rm3ps",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ACCEPTANCE CHECKS\n",
      "============================================================\n",
      "\n",
      "✓ Check 1: Average predictions and cardinality range\n",
      "  Average K: 1.51\n",
      "  K values: [np.int64(1), np.int64(2), np.int64(3)]\n",
      "  Expected: [1, 2, 3]\n",
      "  Result: PASS\n",
      "\n",
      "✓ Check 2: Gold labels not used during inference\n",
      "  Code verification: predict_for_artifact() does NOT reference gold_controls\n",
      "  Gold controls only attached for evaluation\n",
      "  Result: PASS\n",
      "\n",
      "✓ Check 3: Output file exists\n",
      "  Path: ../outputs/predictions/test.csv\n",
      "  Exists: True\n",
      "  Size: 76.09 KB\n",
      "  Result: PASS\n",
      "\n",
      "============================================================\n",
      "✅ ALL ACCEPTANCE CHECKS PASSED\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"ACCEPTANCE CHECKS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check 1: Average predictions between 1 and ~2.x, all lengths in {1,2,3}\n",
    "avg_k = predictions_df[\"n_predicted\"].mean()\n",
    "valid_k = set(predictions_df[\"n_predicted\"].unique())\n",
    "expected_k = {1, 2, 3}\n",
    "check1 = (1.0 <= avg_k <= 3.0) and valid_k.issubset(expected_k)\n",
    "print(f\"\\n✓ Check 1: Average predictions and cardinality range\")\n",
    "print(f\"  Average K: {avg_k:.2f}\")\n",
    "print(f\"  K values: {sorted(valid_k)}\")\n",
    "print(f\"  Expected: {sorted(expected_k)}\")\n",
    "print(f\"  Result: {'PASS' if check1 else 'FAIL'}\")\n",
    "\n",
    "# Check 2: Gold labels not used during inference\n",
    "# (This is a code review check - we verify the pipeline doesn't use gold_controls for ranking)\n",
    "print(f\"\\n✓ Check 2: Gold labels not used during inference\")\n",
    "print(f\"  Code verification: predict_for_artifact() does NOT reference gold_controls\")\n",
    "print(f\"  Gold controls only attached for evaluation\")\n",
    "check2 = True\n",
    "print(f\"  Result: {'PASS' if check2 else 'FAIL'}\")\n",
    "\n",
    "# Check 3: Output file exists\n",
    "check3 = output_path.exists()\n",
    "print(f\"\\n✓ Check 3: Output file exists\")\n",
    "print(f\"  Path: {output_path}\")\n",
    "print(f\"  Exists: {check3}\")\n",
    "print(f\"  Size: {output_path.stat().st_size / 1024:.2f} KB\" if check3 else \"  Size: N/A\")\n",
    "print(f\"  Result: {'PASS' if check3 else 'FAIL'}\")\n",
    "\n",
    "# Overall\n",
    "all_checks_passed = check1 and check2 and check3\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "if all_checks_passed:\n",
    "    print(\"✅ ALL ACCEPTANCE CHECKS PASSED\")\n",
    "else:\n",
    "    print(\"❌ SOME ACCEPTANCE CHECKS FAILED\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
