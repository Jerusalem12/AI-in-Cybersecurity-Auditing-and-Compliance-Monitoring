{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# 03 â€” Model Comparison: TF-IDF vs Embeddings + Post-Rules\n",
    "\n",
    "This notebook compares:\n",
    "1. **TF-IDF baseline** (no rules)\n",
    "2. **TF-IDF + post-rules** (explainable keyword boosts)\n",
    "3. **Embeddings baseline** (semantic similarity)\n",
    "4. **Embeddings + post-rules** (best of both worlds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": "import yaml\nfrom pathlib import Path\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom crs.dataio import load_artifacts, load_controls\nfrom crs.controls import build_index_text\nfrom crs.recommenders.tfidf import TFIDFRecommender\nfrom crs.recommenders.embeddings import EmbeddingRecommender\nfrom crs.postrules import apply_post_rules, Rule\nfrom crs.metrics import top1_accuracy, precision_at_k, recall_at_k, jaccard\n\n# Load config (use tfidf.yaml as reference)\nCFG_PATH = Path('../configs/tfidf.yaml')\ncfg = yaml.safe_load(CFG_PATH.read_text())\n\n# Resolve paths relative to project root\nproject_root = Path('..').resolve()\ncontrols = load_controls(project_root / cfg['paths']['controls'])\nartifacts = load_artifacts(project_root / cfg['paths']['artifacts'])\n\n# Build index\nindex_texts = build_index_text(controls)\ncontrol_ids = controls['control_id'].tolist()\n\n# Load rules\nrules_cfg = yaml.safe_load((project_root / cfg['paths']['rules']).read_text())\nrules = [\n    Rule(\n        control_id=r['control_id'],\n        any_keywords=r.get('any_keywords', []),\n        all_keywords=r.get('all_keywords', []),\n        negative_keywords=r.get('negative_keywords', []),\n        boost=float(r.get('boost', 0.08)),\n        dampen=float(r.get('dampen', 0.06)),\n    )\n    for r in rules_cfg.get('rules', [])\n]\n\nprint(f\"Loaded {len(controls)} controls, {len(artifacts)} artifacts\")\nprint(f\"Loaded {len(rules)} post-rules\")"
  },
  {
   "cell_type": "markdown",
   "id": "tfidf_section",
   "metadata": {},
   "source": [
    "## 1. TF-IDF Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tfidf_baseline",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train TF-IDF\n",
    "tfidf_rec = TFIDFRecommender(\n",
    "    ngram_range=tuple(cfg.get('tfidf', {}).get('ngram_range', [1,2])),\n",
    "    min_df=cfg.get('tfidf', {}).get('min_df', 1)\n",
    ").fit(index_texts, control_ids)\n",
    "\n",
    "k = cfg.get('k', 3)\n",
    "test = artifacts[artifacts['split']=='test'].copy()\n",
    "\n",
    "# TF-IDF baseline (no rules)\n",
    "rows_tfidf_base = []\n",
    "for _, r in test.iterrows():\n",
    "    ids, scores = tfidf_rec.predict_topk(r['text'], k=k)\n",
    "    rows_tfidf_base.append({\n",
    "        'artifact_id': int(r['artifact_id']),\n",
    "        'text': r['text'],\n",
    "        'gold_controls': r['gold_controls'],\n",
    "        'predicted_topk': ';'.join(ids),\n",
    "        'scores_topk': ';'.join(f\"{s:.4f}\" for s in scores),\n",
    "    })\n",
    "preds_tfidf_base = pd.DataFrame(rows_tfidf_base)\n",
    "\n",
    "# TF-IDF + post-rules\n",
    "rows_tfidf_rules = []\n",
    "for _, r in test.iterrows():\n",
    "    ids, scores = tfidf_rec.predict_topk(r['text'], k=k)\n",
    "    ids, scores, notes = apply_post_rules(r['text'], ids, scores, rules)\n",
    "    ids, scores = ids[:k], scores[:k]\n",
    "    rows_tfidf_rules.append({\n",
    "        'artifact_id': int(r['artifact_id']),\n",
    "        'text': r['text'],\n",
    "        'gold_controls': r['gold_controls'],\n",
    "        'predicted_topk': ';'.join(ids),\n",
    "        'scores_topk': ';'.join(f\"{s:.4f}\" for s in scores),\n",
    "        'postrule_notes': ' | '.join(notes)\n",
    "    })\n",
    "preds_tfidf_rules = pd.DataFrame(rows_tfidf_rules)\n",
    "\n",
    "print(\"TF-IDF Baseline:\")\n",
    "print(f\"  Top-1 Accuracy: {top1_accuracy(preds_tfidf_base):.3f}\")\n",
    "print(f\"  P@{k}: {precision_at_k(preds_tfidf_base, k=k):.3f}\")\n",
    "print(f\"  R@{k}: {recall_at_k(preds_tfidf_base, k=k):.3f}\")\n",
    "print(f\"  Jaccard@{k}: {jaccard(preds_tfidf_base, k=k):.3f}\")\n",
    "\n",
    "print(\"\\nTF-IDF + Post-Rules:\")\n",
    "print(f\"  Top-1 Accuracy: {top1_accuracy(preds_tfidf_rules):.3f}\")\n",
    "print(f\"  P@{k}: {precision_at_k(preds_tfidf_rules, k=k):.3f}\")\n",
    "print(f\"  R@{k}: {recall_at_k(preds_tfidf_rules, k=k):.3f}\")\n",
    "print(f\"  Jaccard@{k}: {jaccard(preds_tfidf_rules, k=k):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "embeddings_section",
   "metadata": {},
   "source": [
    "## 2. Embeddings Models\n",
    "\n",
    "**Note:** This will use sentence-transformers if installed (`pip install -e .[embeddings]`), otherwise falls back to a deterministic hash-based embedding for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "embeddings",
   "metadata": {},
   "outputs": [],
   "source": "# Load embeddings config for proper model name\nemb_cfg = yaml.safe_load((project_root / 'configs/embeddings.yaml').read_text())\n\n# Train Embeddings\nemb_rec = EmbeddingRecommender(\n    model_name=emb_cfg.get('embeddings', {}).get('model_name')\n).fit(index_texts, control_ids)\n\n# Embeddings baseline (no rules)\nrows_emb_base = []\nfor _, r in test.iterrows():\n    ids, scores = emb_rec.predict_topk(r['text'], k=k)\n    rows_emb_base.append({\n        'artifact_id': int(r['artifact_id']),\n        'text': r['text'],\n        'gold_controls': r['gold_controls'],\n        'predicted_topk': ';'.join(ids),\n        'scores_topk': ';'.join(f\"{s:.4f}\" for s in scores),\n    })\npreds_emb_base = pd.DataFrame(rows_emb_base)\n\n# Embeddings + post-rules\nrows_emb_rules = []\nfor _, r in test.iterrows():\n    ids, scores = emb_rec.predict_topk(r['text'], k=k)\n    ids, scores, notes = apply_post_rules(r['text'], ids, scores, rules)\n    ids, scores = ids[:k], scores[:k]\n    rows_emb_rules.append({\n        'artifact_id': int(r['artifact_id']),\n        'text': r['text'],\n        'gold_controls': r['gold_controls'],\n        'predicted_topk': ';'.join(ids),\n        'scores_topk': ';'.join(f\"{s:.4f}\" for s in scores),\n        'postrule_notes': ' | '.join(notes)\n    })\npreds_emb_rules = pd.DataFrame(rows_emb_rules)\n\nprint(\"Embeddings Baseline:\")\nprint(f\"  Top-1 Accuracy: {top1_accuracy(preds_emb_base):.3f}\")\nprint(f\"  P@{k}: {precision_at_k(preds_emb_base, k=k):.3f}\")\nprint(f\"  R@{k}: {recall_at_k(preds_emb_base, k=k):.3f}\")\nprint(f\"  Jaccard@{k}: {jaccard(preds_emb_base, k=k):.3f}\")\n\nprint(\"\\nEmbeddings + Post-Rules:\")\nprint(f\"  Top-1 Accuracy: {top1_accuracy(preds_emb_rules):.3f}\")\nprint(f\"  P@{k}: {precision_at_k(preds_emb_rules, k=k):.3f}\")\nprint(f\"  R@{k}: {recall_at_k(preds_emb_rules, k=k):.3f}\")\nprint(f\"  Jaccard@{k}: {jaccard(preds_emb_rules, k=k):.3f}\")"
  },
  {
   "cell_type": "markdown",
   "id": "comparison",
   "metadata": {},
   "source": [
    "## 3. Side-by-Side Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparison_table",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table\n",
    "comparison = pd.DataFrame([\n",
    "    {\n",
    "        'Model': 'TF-IDF',\n",
    "        'Post-Rules': 'No',\n",
    "        'Top-1 Acc': top1_accuracy(preds_tfidf_base),\n",
    "        f'P@{k}': precision_at_k(preds_tfidf_base, k=k),\n",
    "        f'R@{k}': recall_at_k(preds_tfidf_base, k=k),\n",
    "        f'Jaccard@{k}': jaccard(preds_tfidf_base, k=k)\n",
    "    },\n",
    "    {\n",
    "        'Model': 'TF-IDF',\n",
    "        'Post-Rules': 'Yes',\n",
    "        'Top-1 Acc': top1_accuracy(preds_tfidf_rules),\n",
    "        f'P@{k}': precision_at_k(preds_tfidf_rules, k=k),\n",
    "        f'R@{k}': recall_at_k(preds_tfidf_rules, k=k),\n",
    "        f'Jaccard@{k}': jaccard(preds_tfidf_rules, k=k)\n",
    "    },\n",
    "    {\n",
    "        'Model': 'Embeddings',\n",
    "        'Post-Rules': 'No',\n",
    "        'Top-1 Acc': top1_accuracy(preds_emb_base),\n",
    "        f'P@{k}': precision_at_k(preds_emb_base, k=k),\n",
    "        f'R@{k}': recall_at_k(preds_emb_base, k=k),\n",
    "        f'Jaccard@{k}': jaccard(preds_emb_base, k=k)\n",
    "    },\n",
    "    {\n",
    "        'Model': 'Embeddings',\n",
    "        'Post-Rules': 'Yes',\n",
    "        'Top-1 Acc': top1_accuracy(preds_emb_rules),\n",
    "        f'P@{k}': precision_at_k(preds_emb_rules, k=k),\n",
    "        f'R@{k}': recall_at_k(preds_emb_rules, k=k),\n",
    "        f'Jaccard@{k}': jaccard(preds_emb_rules, k=k)\n",
    "    }\n",
    "])\n",
    "\n",
    "display(comparison.round(3))\n",
    "\n",
    "# Save comparison\n",
    "OUT_DIR = Path('../eval/tables')\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "comparison.to_csv(OUT_DIR / 'model_comparison.csv', index=False)\n",
    "print(f'\\nSaved comparison to {OUT_DIR / \"model_comparison.csv\"}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viz",
   "metadata": {},
   "source": [
    "## 4. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "metrics = ['Top-1 Acc', f'P@{k}', f'R@{k}', f'Jaccard@{k}']\n",
    "for idx, metric in enumerate(metrics):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    comparison.plot(x='Model', y=metric, kind='bar', ax=ax, legend=False)\n",
    "    ax.set_title(metric)\n",
    "    ax.set_ylabel('Score')\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_xticklabels(['TF-IDF\\n(no rules)', 'TF-IDF\\n(+rules)', 'Embeddings\\n(no rules)', 'Embeddings\\n(+rules)'], rotation=0)\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../eval/tables/model_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('Saved plot to eval/tables/model_comparison.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "examples",
   "metadata": {},
   "source": [
    "## 5. Example Predictions with Post-Rule Explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "example_explanations",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a few examples with rule explanations\n",
    "sample_size = 5\n",
    "sample_preds = preds_tfidf_rules.head(sample_size)[['artifact_id', 'text', 'gold_controls', 'predicted_topk', 'postrule_notes']]\n",
    "\n",
    "for _, row in sample_preds.iterrows():\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Artifact {row['artifact_id']}:\")\n",
    "    print(f\"Text: {row['text'][:100]}...\")\n",
    "    print(f\"Gold: {row['gold_controls']}\")\n",
    "    print(f\"Predicted: {row['predicted_topk']}\")\n",
    "    if row['postrule_notes']:\n",
    "        print(f\"Rules Applied: {row['postrule_notes']}\")\n",
    "    else:\n",
    "        print(\"Rules Applied: None\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}