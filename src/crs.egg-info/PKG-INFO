Metadata-Version: 2.4
Name: crs
Version: 0.1.0
Summary: Compliance Recommendation System: NIST 800-53 control mapping with OSCAL evidence and auditor feedback.
Requires-Python: >=3.10
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: pandas>=2.0
Requires-Dist: numpy>=1.24
Requires-Dist: scikit-learn>=1.3
Requires-Dist: pyyaml>=6.0
Requires-Dist: matplotlib>=3.7
Provides-Extra: embeddings
Requires-Dist: sentence-transformers>=2.6; extra == "embeddings"
Requires-Dist: torch>=2.2; extra == "embeddings"
Dynamic: license-file

# Compliance Recommendation System (CRS)

AI-assisted mapping of operational artifacts to **NIST SP 800-53 r5** controls, with **OSCAL** evidence output and **auditor feedback**.

## Quickstart
```bash
python -m venv .venv && . .venv/bin/activate
pip install -U pip
pip install -e .
# Put your CSVs here:
mkdir -p data/raw
# copy files: data/raw/controls.csv, data/raw/artifacts.csv
make index
make recommend
make eval
```

## Project Structure

```
.
├─ README.md
├─ LICENSE
├─ .gitignore
├─ pyproject.toml
├─ Makefile
├─ .pre-commit-config.yaml
├─ configs/
│  ├─ defaults.yaml
│  └─ eval.yaml
├─ data/              # raw/ (inputs), interim/, processed/
├─ models/            # saved vectorizers/embeddings
├─ src/
│  ├─ crs/            # library code (dataio, recommenders, metrics, oscal)
│  └─ cli/            # scripts: build_index, recommend, evaluate, learn_feedback
├─ outputs/           # predictions, oscal bundles
└─ eval/              # tables and plots
```

## What it does

* **Recommender**: TF-IDF (and optional embeddings) + cosine similarity over control text.
* **Feedback**: accept/reject/add logs � learn boosts/penalties.
* **Metrics**: Top-1 accuracy, P@3/R@3, Jaccard; Acceptance Rate; Time-to-Evidence; Set-based P/R/F1.
* **Evidence**: Exports OSCAL *assessment-results* JSON.
* **Auto-K**: Variable-length predictions using elbow detection and per-type limits.

## Config

See `configs/tfidf.yaml` or `configs/embeddings.yaml` for model configuration.

### Auto-K: Intelligent Variable-Length Predictions

The system uses an **auto-k** algorithm that automatically determines how many controls to recommend for each finding:

**How it works:**
1. **Score threshold**: Keeps controls above `min_score` (e.g., 0.15)
2. **Elbow detection**: Detects sharp drops in similarity scores (e.g., 0.75 → 0.30)
3. **Clamping**: Ensures results stay within `min_k` to `max_k` bounds
4. **Per-type limits**: Different caps for logs (3), configs (2), tickets (2)

**Configuration** ([tfidf.yaml](configs/tfidf.yaml)):
```yaml
recommend:
  candidate_k: 10        # Initial pool size
  auto_k:
    enabled: true
    min_k: 1             # Never return fewer than this
    max_k: 5             # Cap to avoid spam
    min_score: 0.15      # Absolute score threshold
    drop_ratio: 0.25     # Elbow: cut where score drops ≥25%
    per_type_max_k:
      log: 3
      config: 2
      ticket: 2
```

**Results:**
- Finding with 1 highly relevant control → returns 1
- Finding with clear top-3 (elbow at 3) → returns 3
- Finding with many similar controls → capped at max_k

**Command-line override:**
```bash
python -m cli.recommend --config configs/tfidf.yaml --in data/raw/artifacts.csv --out outputs/predictions/test.csv --candidate_k 15
```

### Set-Based Metrics

When using auto-k, use set-based metrics that respect variable-length predictions:

```bash
python -m cli.evaluate --config configs/eval.yaml --pred outputs/predictions/test.csv --set_metrics
```

This computes:
- **set_precision**: |gold ∩ pred| / |pred| (average across all findings)
- **set_recall**: |gold ∩ pred| / |gold| (average across all findings)
- **set_f1**: Harmonic mean of set precision and recall

These metrics don't require choosing a fixed k value.

## Citation

Based on class project guidance and *Towards Automated Continuous Security Compliance (ESEM '24)* for problem framing.
